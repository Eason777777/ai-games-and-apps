{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"ç®¡ç†è¨“ç·´éç¨‹ä¸­çš„checkpointä¿å­˜å’Œè¼‰å…¥\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, model, optimizer, iteration, total_iterations, \n",
    "                       training_history, metadata=None):\n",
    "        \"\"\"ä¿å­˜è¨“ç·´checkpoint\"\"\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_name = f\"checkpoint_iter_{iteration:03d}_{timestamp}.pt\"\n",
    "        checkpoint_path = self.checkpoint_dir / checkpoint_name\n",
    "        \n",
    "        checkpoint_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'iteration': iteration,\n",
    "            'total_iterations': total_iterations,\n",
    "            'training_history': training_history,\n",
    "            'timestamp': timestamp,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint_data, checkpoint_path)\n",
    "        \n",
    "        # åŒæ™‚ä¿å­˜æœ€æ–°çš„checkpointç‚ºå›ºå®šåç¨±\n",
    "        latest_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "        torch.save(checkpoint_data, latest_path)\n",
    "        \n",
    "        # ä¿å­˜è¨“ç·´æ­·å²ç‚ºå–®ç¨æ–‡ä»¶\n",
    "        history_path = self.checkpoint_dir / f\"history_iter_{iteration:03d}.pkl\"\n",
    "        with open(history_path, 'wb') as f:\n",
    "            pickle.dump(training_history, f)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Checkpoint å·²ä¿å­˜: {checkpoint_name}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_latest_checkpoint(self, model, optimizer):\n",
    "        \"\"\"è¼‰å…¥æœ€æ–°çš„checkpoint\"\"\"\n",
    "        latest_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "        \n",
    "        if not latest_path.exists():\n",
    "            print(\"ğŸ“­ æœªæ‰¾åˆ°checkpointï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(latest_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "            print(f\"ğŸ“¥ å·²è¼‰å…¥checkpoint: è¿­ä»£ {checkpoint['iteration']}/{checkpoint['total_iterations']}\")\n",
    "            print(f\"ğŸ“… ä¿å­˜æ™‚é–“: {checkpoint['timestamp']}\")\n",
    "            \n",
    "            return checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¼‰å…¥checkpointå¤±æ•—: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„checkpoint\"\"\"\n",
    "        checkpoint_files = list(self.checkpoint_dir.glob(\"checkpoint_iter_*.pt\"))\n",
    "        checkpoint_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        return checkpoint_files\n",
    "    \n",
    "    def clean_old_checkpoints(self, keep_count=5):\n",
    "        \"\"\"æ¸…ç†èˆŠçš„checkpointï¼Œåªä¿ç•™æœ€æ–°çš„å¹¾å€‹\"\"\"\n",
    "        checkpoint_files = self.list_checkpoints()\n",
    "        \n",
    "        if len(checkpoint_files) > keep_count:\n",
    "            files_to_delete = checkpoint_files[keep_count:]\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    # åŒæ™‚åˆªé™¤å°æ‡‰çš„æ­·å²æ–‡ä»¶\n",
    "                    history_file = file_path.parent / file_path.name.replace(\"checkpoint_\", \"history_\")\n",
    "                    if history_file.exists():\n",
    "                        os.remove(history_file)\n",
    "                    print(f\"ğŸ—‘ï¸  å·²æ¸…ç†èˆŠcheckpoint: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  æ¸…ç†checkpointå¤±æ•— {file_path.name}: {e}\")\n",
    "    \n",
    "    def get_checkpoint_info(self):\n",
    "        \"\"\"ç²å–checkpointä¿¡æ¯\"\"\"\n",
    "        latest_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "        \n",
    "        if not latest_path.exists():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(latest_path, map_location='cpu')\n",
    "            return {\n",
    "                'iteration': checkpoint['iteration'],\n",
    "                'total_iterations': checkpoint['total_iterations'],\n",
    "                'timestamp': checkpoint['timestamp'],\n",
    "                'metadata': checkpoint.get('metadata', {}),\n",
    "                'training_history': checkpoint.get('training_history', {})\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è®€å–checkpointä¿¡æ¯å¤±æ•—: {e}\")\n",
    "            return None\n",
    "\n",
    "# åˆå§‹åŒ–checkpointç®¡ç†å™¨\n",
    "checkpoint_manager = CheckpointManager(CHECKPOINT_DIR)\n",
    "print(f\"ğŸ“‚ Checkpointç®¡ç†å™¨å·²åˆå§‹åŒ–: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f104552",
   "metadata": {},
   "source": [
    "# é»‘ç™½æ£‹ (Reversi) é«˜å‹ç‡ AI è¨“ç·´\n",
    "\n",
    "æœ¬ç­†è¨˜æœ¬ç”¨æ–¼è¨“ç·´ä¸€å€‹éå¸¸ä¸å®¹æ˜“è¼¸çš„é»‘ç™½æ£‹ AI æ¨¡å‹ï¼Œè¨“ç·´å®Œæˆå¾Œå°‡æ¨¡å‹è½‰æ›ç‚º JavaScript æ ¼å¼ä»¥ä¾¿åœ¨ Web æ‡‰ç”¨ç¨‹å¼ä¸­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e968e2e",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒè¨­ç½®\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦ç¢ºèª GPU å¯ç”¨ï¼Œä¸¦å®‰è£å¿…è¦çš„å‡½å¼åº«ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU æ˜¯å¦å¯ç”¨: False\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥ GPU æ˜¯å¦å¯ç”¨\n",
    "import torch\n",
    "print(\"GPU æ˜¯å¦å¯ç”¨:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU å‹è™Ÿ:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# å®‰è£å¿…è¦çš„å‡½å¼åº«\n",
    "!pip install numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ GPU æ˜¯å¦å¯ç”¨\n",
    "import torch\n",
    "print(\"GPU æ˜¯å¦å¯ç”¨:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU å‹è™Ÿ:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# å®‰è£å¿…è¦çš„å‡½å¼åº«\n",
    "!pip install numpy matplotlib tqdm\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦åœ¨ Colab ç’°å¢ƒä¸­é‹è¡Œ\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"åœ¨ Google Colab ç’°å¢ƒä¸­é‹è¡Œ\")\n",
    "    # åœ¨ Colab ä¸­æ›è¼‰ Google Drive ä»¥ä¿å­˜ checkpoint\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/reversi_checkpoints/'\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"åœ¨æœ¬åœ°ç’°å¢ƒä¸­é‹è¡Œ\")\n",
    "    CHECKPOINT_DIR = './checkpoints/'\n",
    "\n",
    "# å‰µå»º checkpoint ç›®éŒ„\n",
    "import os\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoint å°‡ä¿å­˜åˆ°: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206e740",
   "metadata": {},
   "source": [
    "## é»‘ç™½æ£‹éŠæˆ²å¯¦ç¾\n",
    "\n",
    "æ¥ä¸‹ä¾†æˆ‘å€‘å¯¦ç¾é»‘ç™½æ£‹çš„éŠæˆ²è¦å‰‡ï¼ŒåŒ…æ‹¬æ£‹ç›¤åˆå§‹åŒ–ã€èµ°æ­¥é©—è­‰ã€æ£‹å­ç¿»è½‰ç­‰åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7db255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class ReversiGame:\n",
    "    def __init__(self):\n",
    "        # åˆå§‹åŒ– 8x8 æ£‹ç›¤ï¼Œ0=ç©ºï¼Œ1=é»‘ï¼Œ2=ç™½\n",
    "        self.board = np.zeros((8, 8), dtype=np.int32)\n",
    "        \n",
    "        # è¨­ç½®åˆå§‹å››å€‹æ£‹å­\n",
    "        self.board[3][3] = 2  # ç™½\n",
    "        self.board[3][4] = 1  # é»‘\n",
    "        self.board[4][3] = 1  # é»‘\n",
    "        self.board[4][4] = 2  # ç™½\n",
    "        \n",
    "        # ç•¶å‰ç©å®¶ (1=é»‘, 2=ç™½)\n",
    "        self.current_player = 1\n",
    "        \n",
    "        # éŠæˆ²æ˜¯å¦çµæŸ\n",
    "        self.game_over = False\n",
    "        \n",
    "        # æ–¹å‘å‘é‡\n",
    "        self.directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), \n",
    "                         (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "    \n",
    "    def clone(self):\n",
    "        \"\"\"è¤‡è£½ç•¶å‰éŠæˆ²ç‹€æ…‹\"\"\"\n",
    "        new_game = ReversiGame()\n",
    "        new_game.board = self.board.copy()\n",
    "        new_game.current_player = self.current_player\n",
    "        new_game.game_over = self.game_over\n",
    "        return new_game\n",
    "    \n",
    "    def is_valid_move(self, row, col):\n",
    "        \"\"\"æª¢æŸ¥ç•¶å‰ç©å®¶åœ¨æŒ‡å®šä½ç½®æ˜¯å¦å¯ä»¥ä¸‹æ£‹\"\"\"\n",
    "        # å¦‚æœæ ¼å­å·²è¢«ä½”ç”¨ï¼Œå‰‡ä¸åˆæ³•\n",
    "        if self.board[row][col] != 0:\n",
    "            return False\n",
    "        \n",
    "        # å°æ‰‹çš„æ£‹å­é¡è‰²\n",
    "        opponent = 3 - self.current_player\n",
    "        \n",
    "        # æª¢æŸ¥å…«å€‹æ–¹å‘\n",
    "        for dr, dc in self.directions:\n",
    "            r, c = row + dr, col + dc\n",
    "            # ç¢ºèªæœ‰å°æ‰‹çš„æ£‹å­ç›¸é„°\n",
    "            if 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == opponent:\n",
    "                # æ²¿è‘—é€™å€‹æ–¹å‘ç¹¼çºŒå‰é€²\n",
    "                r, c = r + dr, c + dc\n",
    "                while 0 <= r < 8 and 0 <= c < 8:\n",
    "                    # å¦‚æœé‡åˆ°ç©ºæ ¼ï¼Œé€™å€‹æ–¹å‘ç„¡æ³•ç¿»è½‰æ£‹å­\n",
    "                    if self.board[r][c] == 0:\n",
    "                        break\n",
    "                    # å¦‚æœé‡åˆ°è‡ªå·±çš„æ£‹å­ï¼Œå‰‡å¯ä»¥ç¿»è½‰ï¼Œèµ°æ­¥åˆæ³•\n",
    "                    if self.board[r][c] == self.current_player:\n",
    "                        return True\n",
    "                    # ç¹¼çºŒæ²¿è‘—é€™å€‹æ–¹å‘\n",
    "                    r, c = r + dr, c + dc\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_valid_moves(self):\n",
    "        \"\"\"ç²å–ç•¶å‰ç©å®¶çš„æ‰€æœ‰åˆæ³•èµ°æ­¥\"\"\"\n",
    "        valid_moves = []\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                if self.is_valid_move(row, col):\n",
    "                    valid_moves.append((row, col))\n",
    "        return valid_moves\n",
    "    \n",
    "    def make_move(self, row, col):\n",
    "        \"\"\"åŸ·è¡Œä¸€æ­¥èµ°æ­¥\"\"\"\n",
    "        if not self.is_valid_move(row, col):\n",
    "            return False\n",
    "        \n",
    "        # æ”¾ç½®æ£‹å­\n",
    "        self.board[row][col] = self.current_player\n",
    "        \n",
    "        # ç¿»è½‰å°æ‰‹çš„æ£‹å­\n",
    "        opponent = 3 - self.current_player\n",
    "        for dr, dc in self.directions:\n",
    "            pieces_to_flip = []\n",
    "            r, c = row + dr, col + dc\n",
    "            \n",
    "            # æª¢æŸ¥è©²æ–¹å‘æ˜¯å¦æœ‰å¯ä»¥ç¿»è½‰çš„æ£‹å­\n",
    "            while 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == opponent:\n",
    "                pieces_to_flip.append((r, c))\n",
    "                r, c = r + dr, c + dc\n",
    "                \n",
    "                # å¦‚æœé‡åˆ°è‡ªå·±çš„æ£‹å­ï¼Œç¿»è½‰ä¸­é–“æ‰€æœ‰å°æ‰‹çš„æ£‹å­\n",
    "                if 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == self.current_player:\n",
    "                    for flip_r, flip_c in pieces_to_flip:\n",
    "                        self.board[flip_r][flip_c] = self.current_player\n",
    "                    break\n",
    "        \n",
    "        # åˆ‡æ›ç©å®¶\n",
    "        self.current_player = opponent\n",
    "        \n",
    "        # æª¢æŸ¥ä¸‹ä¸€å€‹ç©å®¶æ˜¯å¦æœ‰åˆæ³•èµ°æ­¥\n",
    "        if not self.get_valid_moves():\n",
    "            # å¦‚æœä¸‹ä¸€å€‹ç©å®¶æ²’æœ‰åˆæ³•èµ°æ­¥ï¼Œæª¢æŸ¥ç•¶å‰ç©å®¶æ˜¯å¦æœ‰åˆæ³•èµ°æ­¥\n",
    "            self.current_player = 3 - self.current_player\n",
    "            if not self.get_valid_moves():\n",
    "                # å¦‚æœé›™æ–¹éƒ½æ²’æœ‰åˆæ³•èµ°æ­¥ï¼ŒéŠæˆ²çµæŸ\n",
    "                self.game_over = True\n",
    "            # å¦å‰‡è·³éç•¶å‰ç©å®¶\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_score(self):\n",
    "        \"\"\"ç²å–ç•¶å‰åˆ†æ•¸\"\"\"\n",
    "        black_count = np.sum(self.board == 1)\n",
    "        white_count = np.sum(self.board == 2)\n",
    "        return black_count, white_count\n",
    "    \n",
    "    def get_winner(self):\n",
    "        \"\"\"ç²å–è´å®¶ (1=é»‘, 2=ç™½, 0=å¹³å±€)\"\"\"\n",
    "        black_count, white_count = self.get_score()\n",
    "        if black_count > white_count:\n",
    "            return 1\n",
    "        elif white_count > black_count:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def print_board(self):\n",
    "        \"\"\"æ‰“å°æ£‹ç›¤\"\"\"\n",
    "        symbols = {0: '.', 1: 'â—', 2: 'â—‹'}\n",
    "        print('  0 1 2 3 4 5 6 7')\n",
    "        for i in range(8):\n",
    "            row_str = f\"{i} \"\n",
    "            for j in range(8):\n",
    "                row_str += symbols[self.board[i][j]] + ' '\n",
    "            print(row_str)\n",
    "        black, white = self.get_score()\n",
    "        print(f\"é»‘æ£‹: {black}, ç™½æ£‹: {white}\")\n",
    "        print(f\"ç•¶å‰ç©å®¶: {'é»‘' if self.current_player == 1 else 'ç™½'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦éŠæˆ²å¯¦ç¾\n",
    "game = ReversiGame()\n",
    "game.print_board()\n",
    "\n",
    "# é¡¯ç¤ºåˆæ³•èµ°æ­¥\n",
    "valid_moves = game.get_valid_moves()\n",
    "print(f\"åˆæ³•èµ°æ­¥: {valid_moves}\")\n",
    "\n",
    "# åŸ·è¡Œä¸€æ­¥èµ°æ­¥\n",
    "if valid_moves:\n",
    "    game.make_move(valid_moves[0][0], valid_moves[0][1])\n",
    "    print(\"\\nåŸ·è¡Œèµ°æ­¥å¾Œ:\")\n",
    "    game.print_board()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef6190",
   "metadata": {},
   "source": [
    "## ç¥ç¶“ç¶²è·¯æ¨¡å‹\n",
    "\n",
    "æˆ‘å€‘å°‡å»ºç«‹ä¸€å€‹æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œè©²æ¨¡å‹æ¥æ”¶æ£‹ç›¤ç‹€æ…‹ä½œç‚ºè¼¸å…¥ï¼Œä¸¦è¼¸å‡ºæ¯å€‹ä½ç½®çš„èµ°æ­¥åƒ¹å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5121ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReversiNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReversiNet, self).__init__()\n",
    "        # æ£‹ç›¤ç‹€æ…‹è¼¸å…¥: 3 é€šé“ (è‡ªå·±çš„æ£‹å­ã€å°æ‰‹çš„æ£‹å­ã€ç©ºä½) x 8 x 8\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # æ®˜å·®é€£æ¥\n",
    "        self.residual1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        self.residual2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        # ç­–ç•¥é ­ (èµ°æ­¥æ¦‚ç‡)\n",
    "        self.policy_conv = nn.Conv2d(128, 32, kernel_size=1)\n",
    "        self.policy_fc = nn.Linear(32 * 8 * 8, 64)  # 8x8 æ£‹ç›¤çš„æ‰€æœ‰ä½ç½®\n",
    "        \n",
    "        # åƒ¹å€¼é ­ (å‹ç‡è©•ä¼°)\n",
    "        self.value_conv = nn.Conv2d(128, 32, kernel_size=1)\n",
    "        self.value_fc1 = nn.Linear(32 * 8 * 8, 64)\n",
    "        self.value_fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "        # Batch Normalization å±¤\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn_policy = nn.BatchNorm2d(32)\n",
    "        self.bn_value = nn.BatchNorm2d(32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # å·ç©å±¤\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # æ®˜å·®é€£æ¥\n",
    "        x_res = x\n",
    "        x = F.relu(x + self.residual1(x_res))\n",
    "        x_res = x\n",
    "        x = F.relu(x + self.residual2(x_res))\n",
    "        \n",
    "        # ç­–ç•¥é ­\n",
    "        policy = F.relu(self.bn_policy(self.policy_conv(x)))\n",
    "        policy = policy.view(-1, 32 * 8 * 8)\n",
    "        policy = self.policy_fc(policy)\n",
    "        \n",
    "        # åƒ¹å€¼é ­\n",
    "        value = F.relu(self.bn_value(self.value_conv(x)))\n",
    "        value = value.view(-1, 32 * 8 * 8)\n",
    "        value = F.relu(self.value_fc1(value))\n",
    "        value = torch.tanh(self.value_fc2(value))  # è¼¸å‡ºç¯„åœ [-1, 1]\n",
    "        \n",
    "        return policy, value\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ReversiNet().to(device)\n",
    "print(f\"æ¨¡å‹å·²å»ºç«‹ï¼Œä½¿ç”¨è£ç½®: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3cafb",
   "metadata": {},
   "source": [
    "## è‡ªæˆ‘å°å¼ˆèˆ‡å¼·åŒ–å­¸ç¿’\n",
    "\n",
    "æ¥ä¸‹ä¾†æˆ‘å€‘ä½¿ç”¨å¼·åŒ–å­¸ç¿’ä¸­çš„è’™åœ°å¡ç¾…æ¨¹æœç´¢ (MCTS) ä¾†é€šéè‡ªæˆ‘å°å¼ˆç”Ÿæˆè¨“ç·´æ•¸æ“šï¼Œç„¶å¾Œä½¿ç”¨é€™äº›æ•¸æ“šè¨“ç·´ç¥ç¶“ç¶²è·¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51350ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, game, parent=None, move=None):\n",
    "        self.game = game\n",
    "        self.parent = parent\n",
    "        self.move = move  # åˆ°é”æ­¤ç¯€é»çš„èµ°æ­¥\n",
    "        self.children = {}  # å­ç¯€é»\n",
    "        self.visits = 0  # è¨ªå•æ¬¡æ•¸\n",
    "        self.value = 0.0  # ç¸½çå‹µ\n",
    "        self.untried_moves = game.get_valid_moves()  # æœªå˜—è©¦çš„èµ°æ­¥\n",
    "    \n",
    "    def select_child(self, c_param=1.4):\n",
    "        \"\"\"é¸æ“‡æœ€å…·æ½›åŠ›çš„å­ç¯€é» (UCB1)\"\"\"\n",
    "        # é¸æ“‡ UCB å€¼æœ€é«˜çš„å­ç¯€é»\n",
    "        return max(self.children.values(),\n",
    "                  key=lambda node: node.value / (node.visits + 1e-8) + \\\n",
    "                  c_param * math.sqrt(2 * math.log(self.visits + 1) / (node.visits + 1e-8)))\n",
    "    \n",
    "    def expand(self):\n",
    "        \"\"\"æ“´å±•ä¸€å€‹æœªå˜—è©¦çš„èµ°æ­¥\"\"\"\n",
    "        row, col = self.untried_moves.pop()\n",
    "        next_game = self.game.clone()\n",
    "        next_game.make_move(row, col)\n",
    "        child_node = MCTSNode(next_game, parent=self, move=(row, col))\n",
    "        self.children[(row, col)] = child_node\n",
    "        return child_node\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        \"\"\"æª¢æŸ¥æ˜¯å¦æ‰€æœ‰å¯èƒ½çš„èµ°æ­¥éƒ½å·²å˜—è©¦\"\"\"\n",
    "        return len(self.untried_moves) == 0\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        \"\"\"æª¢æŸ¥æ˜¯å¦ç‚ºçµ‚æ­¢ç¯€é»\"\"\"\n",
    "        return self.game.game_over\n",
    "    \n",
    "    def rollout(self, model=None):\n",
    "        \"\"\"éš¨æ©Ÿæ¨¡æ“¬éŠæˆ²ç›´è‡³çµæŸ\"\"\"\n",
    "        # è¤‡è£½ç•¶å‰éŠæˆ²é€²è¡Œæ¨¡æ“¬\n",
    "        sim_game = self.game.clone()\n",
    "        \n",
    "        # ä½¿ç”¨ç¥ç¶“ç¶²è·¯æŒ‡å°çš„èµ°æ­¥é¸æ“‡\n",
    "        if model is not None and not sim_game.game_over:\n",
    "            while not sim_game.game_over:\n",
    "                valid_moves = sim_game.get_valid_moves()\n",
    "                if not valid_moves:\n",
    "                    # åˆ‡æ›ç©å®¶\n",
    "                    sim_game.current_player = 3 - sim_game.current_player\n",
    "                    valid_moves = sim_game.get_valid_moves()\n",
    "                    if not valid_moves:\n",
    "                        sim_game.game_over = True\n",
    "                        break\n",
    "                    continue\n",
    "                \n",
    "                # å°‡æ£‹ç›¤è½‰æ›ç‚ºç¥ç¶“ç¶²è·¯è¼¸å…¥æ ¼å¼\n",
    "                board_tensor = self.prepare_input(sim_game)\n",
    "                \n",
    "                # ç²å–æ¨¡å‹é æ¸¬\n",
    "                policy, _ = model(board_tensor)\n",
    "                \n",
    "                # å°åˆæ³•èµ°æ­¥çš„æ¦‚ç‡é€²è¡Œéæ¿¾\n",
    "                move_probs = torch.zeros(64)\n",
    "                for row, col in valid_moves:\n",
    "                    move_probs[row * 8 + col] = policy[0, row * 8 + col].item()\n",
    "                \n",
    "                # åšSoftmaxç²å–æ¦‚ç‡åˆ†ä½ˆ\n",
    "                move_probs = F.softmax(move_probs, dim=0)\n",
    "                \n",
    "                # æ ¹æ“šæ¦‚ç‡é¸æ“‡èµ°æ­¥\n",
    "                move_idx = torch.multinomial(move_probs, 1).item()\n",
    "                row, col = move_idx // 8, move_idx % 8\n",
    "                \n",
    "                # ç¢ºä¿é¸æ“‡çš„èµ°æ­¥æ˜¯åˆæ³•çš„\n",
    "                if (row, col) in valid_moves:\n",
    "                    sim_game.make_move(row, col)\n",
    "                else:\n",
    "                    # å¦‚æœé¸æ“‡çš„èµ°æ­¥ä¸åˆæ³•ï¼Œå‰‡éš¨æ©Ÿé¸æ“‡ä¸€å€‹åˆæ³•èµ°æ­¥\n",
    "                    row, col = random.choice(valid_moves)\n",
    "                    sim_game.make_move(row, col)\n",
    "        else:\n",
    "            # éš¨æ©Ÿèµ°æ­¥æ¨¡æ“¬\n",
    "            while not sim_game.game_over:\n",
    "                valid_moves = sim_game.get_valid_moves()\n",
    "                if not valid_moves:\n",
    "                    # åˆ‡æ›ç©å®¶\n",
    "                    sim_game.current_player = 3 - sim_game.current_player\n",
    "                    valid_moves = sim_game.get_valid_moves()\n",
    "                    if not valid_moves:\n",
    "                        sim_game.game_over = True\n",
    "                        break\n",
    "                    continue\n",
    "                \n",
    "                # éš¨æ©Ÿé¸æ“‡ä¸€å€‹åˆæ³•èµ°æ­¥\n",
    "                row, col = random.choice(valid_moves)\n",
    "                sim_game.make_move(row, col)\n",
    "        \n",
    "        # ç²å–éŠæˆ²çµæœ\n",
    "        winner = sim_game.get_winner()\n",
    "        \n",
    "        # å¾ç•¶å‰ç©å®¶çš„è§’åº¦è¿”å›çå‹µ\n",
    "        current_player = self.game.current_player\n",
    "        if winner == 0:  # å¹³å±€\n",
    "            return 0.5\n",
    "        elif winner == current_player:  # ç•¶å‰ç©å®¶è´\n",
    "            return 1.0\n",
    "        else:  # ç•¶å‰ç©å®¶è¼¸\n",
    "            return 0.0\n",
    "    \n",
    "    def backpropagate(self, result):\n",
    "        \"\"\"å‘ä¸Šæ›´æ–°çµ±è¨ˆä¿¡æ¯\"\"\"\n",
    "        self.visits += 1\n",
    "        self.value += result\n",
    "        \n",
    "        # å‘ä¸Šå‚³éçµæœ\n",
    "        if self.parent:\n",
    "            # å¾å°æ‰‹çš„è§’åº¦è¨ˆç®—çå‹µï¼ˆç¿»è½‰çµæœï¼‰\n",
    "            self.parent.backpropagate(1 - result)\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_input(game):\n",
    "        \"\"\"å°‡éŠæˆ²ç‹€æ…‹è½‰æ›ç‚ºç¥ç¶“ç¶²è·¯è¼¸å…¥æ ¼å¼\"\"\"\n",
    "        board = game.board\n",
    "        current_player = game.current_player\n",
    "        opponent = 3 - current_player\n",
    "        \n",
    "        # 3 é€šé“: è‡ªå·±çš„æ£‹å­ã€å°æ‰‹çš„æ£‹å­ã€ç©ºä½\n",
    "        player_channel = (board == current_player).astype(np.float32)\n",
    "        opponent_channel = (board == opponent).astype(np.float32)\n",
    "        empty_channel = (board == 0).astype(np.float32)\n",
    "        \n",
    "        # çµ„åˆæˆ 3x8x8 çš„å¼µé‡\n",
    "        state = np.stack([player_channel, opponent_channel, empty_channel])\n",
    "        \n",
    "        # è½‰æ›ç‚º PyTorch å¼µé‡ä¸¦åŠ å…¥æ‰¹æ¬¡ç¶­åº¦\n",
    "        tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        return tensor.to(device)\n",
    "\n",
    "def mcts_search(game, model, num_simulations=800):\n",
    "    \"\"\"åŸ·è¡Œè’™åœ°å¡ç¾…æ¨¹æœç´¢\"\"\"\n",
    "    root = MCTSNode(game)\n",
    "    \n",
    "    for _ in range(num_simulations):\n",
    "        # é¸æ“‡\n",
    "        node = root\n",
    "        while node.is_fully_expanded() and not node.is_terminal():\n",
    "            node = node.select_child()\n",
    "        \n",
    "        # æ“´å±•\n",
    "        if not node.is_terminal() and not node.is_fully_expanded():\n",
    "            node = node.expand()\n",
    "        \n",
    "        # æ¨¡æ“¬\n",
    "        result = node.rollout(model)\n",
    "        \n",
    "        # å›æº¯\n",
    "        node.backpropagate(result)\n",
    "    \n",
    "    # é¸æ“‡è¨ªå•æ¬¡æ•¸æœ€å¤šçš„å­ç¯€é»\n",
    "    if root.children:\n",
    "        best_move = max(root.children.items(),\n",
    "                        key=lambda item: item[1].visits)[0]\n",
    "        return best_move\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def self_play_game(model, temperature=1.0):\n",
    "    \"\"\"é€²è¡Œä¸€å ´è‡ªæˆ‘å°å¼ˆï¼Œç”Ÿæˆè¨“ç·´æ•¸æ“š\"\"\"\n",
    "    game = ReversiGame()\n",
    "    states = []  # è¨˜éŒ„éŠæˆ²ç‹€æ…‹\n",
    "    policies = []  # è¨˜éŒ„èµ°æ­¥æ¦‚ç‡åˆ†ä½ˆ\n",
    "    player_turns = []  # è¨˜éŒ„ç•¶å‰ç©å®¶\n",
    "    \n",
    "    while not game.game_over:\n",
    "        # ç²å–ç•¶å‰ç©å®¶\n",
    "        current_player = game.current_player\n",
    "        player_turns.append(current_player)\n",
    "        \n",
    "        # ç²å–åˆæ³•èµ°æ­¥\n",
    "        valid_moves = game.get_valid_moves()\n",
    "        \n",
    "        # å¦‚æœæ²’æœ‰åˆæ³•èµ°æ­¥ï¼Œåˆ‡æ›ç©å®¶\n",
    "        if not valid_moves:\n",
    "            game.current_player = 3 - game.current_player\n",
    "            continue\n",
    "        \n",
    "        # ä¿å­˜ç•¶å‰ç‹€æ…‹\n",
    "        state = MCTSNode.prepare_input(game).cpu().squeeze(0).numpy()\n",
    "        states.append(state)\n",
    "        \n",
    "        # å°‡éŠæˆ²ç‹€æ…‹é€å…¥æ¨¡å‹ï¼Œç²å–ç­–ç•¥åˆ†ä½ˆå’Œåƒ¹å€¼è©•ä¼°\n",
    "        policy, _ = model(MCTSNode.prepare_input(game))\n",
    "        policy = policy.detach().cpu().squeeze(0).numpy()\n",
    "        \n",
    "        # åŸ·è¡ŒMCTSæœç´¢\n",
    "        move = mcts_search(game, model, num_simulations=400)\n",
    "        \n",
    "        # å‰µå»ºèµ°æ­¥æ¦‚ç‡åˆ†ä½ˆ\n",
    "        mcts_policy = np.zeros(64, dtype=np.float32)\n",
    "        if move is not None:\n",
    "            row, col = move\n",
    "            mcts_policy[row * 8 + col] = 1.0\n",
    "        \n",
    "        policies.append(mcts_policy)\n",
    "        \n",
    "        # åŸ·è¡Œé¸æ“‡çš„èµ°æ­¥\n",
    "        if move is not None:\n",
    "            game.make_move(move[0], move[1])\n",
    "    \n",
    "    # ç²å–æ¯”è³½çµæœ\n",
    "    winner = game.get_winner()\n",
    "    \n",
    "    # è¨ˆç®—æ¯å€‹ç‹€æ…‹çš„çå‹µ\n",
    "    rewards = []\n",
    "    for player in player_turns:\n",
    "        if winner == 0:  # å¹³å±€\n",
    "            rewards.append(0.0)\n",
    "        elif winner == player:  # ç²å‹\n",
    "            rewards.append(1.0)\n",
    "        else:  # å¤±æ•—\n",
    "            rewards.append(-1.0)\n",
    "    \n",
    "    # å°‡æ•¸æ“šè½‰æ›ç‚ºå¼µé‡æ ¼å¼\n",
    "    states = np.array(states)\n",
    "    policies = np.array(policies)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    \n",
    "    return states, policies, rewards\n",
    "\n",
    "def generate_training_data(model, num_games=100):\n",
    "    \"\"\"ç”Ÿæˆè¨“ç·´æ•¸æ“š\"\"\"\n",
    "    all_states = []\n",
    "    all_policies = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    for _ in tqdm(range(num_games)):\n",
    "        states, policies, rewards = self_play_game(model)\n",
    "        all_states.append(states)\n",
    "        all_policies.append(policies)\n",
    "        all_rewards.append(rewards)\n",
    "    \n",
    "    # åˆä½µæ•¸æ“š\n",
    "    all_states = np.concatenate(all_states)\n",
    "    all_policies = np.concatenate(all_policies)\n",
    "    all_rewards = np.concatenate(all_rewards)\n",
    "    \n",
    "    return all_states, all_policies, all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90919b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_model(model, states, policies, rewards, batch_size=64, epochs=5):\n",
    "    \"\"\"è¨“ç·´æ¨¡å‹\"\"\"\n",
    "    # å‰µå»ºæ•¸æ“šé›†\n",
    "    X = torch.FloatTensor(states).to(device)\n",
    "    policy_y = torch.FloatTensor(policies).to(device)\n",
    "    value_y = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X, policy_y, value_y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # å®šç¾©å„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    policy_criterion = nn.MSELoss()\n",
    "    value_criterion = nn.MSELoss()\n",
    "    \n",
    "    # è¨“ç·´å¾ªç’°\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        policy_loss_sum = 0\n",
    "        value_loss_sum = 0\n",
    "        \n",
    "        for batch_x, batch_policy_y, batch_value_y in dataloader:\n",
    "            # å‰å‘å‚³æ’­\n",
    "            policy_pred, value_pred = model(batch_x)\n",
    "            \n",
    "            # è¨ˆç®—æå¤±\n",
    "            policy_loss = policy_criterion(policy_pred, batch_policy_y)\n",
    "            value_loss = value_criterion(value_pred, batch_value_y)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # åå‘å‚³æ’­\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # ç´¯è¨ˆæå¤±\n",
    "            total_loss += loss.item()\n",
    "            policy_loss_sum += policy_loss.item()\n",
    "            value_loss_sum += value_loss.item()\n",
    "        \n",
    "        # æ‰“å°è¨“ç·´é€²åº¦\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_policy_loss = policy_loss_sum / len(dataloader)\n",
    "        avg_value_loss = value_loss_sum / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Policy Loss: {avg_policy_loss:.4f}, Value Loss: {avg_value_loss:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58903a3d",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°\n",
    "\n",
    "ç¾åœ¨æˆ‘å€‘ä¾†å¯¦ç¾æ¨¡å‹çš„è¨“ç·´å’Œè©•ä¼°éç¨‹ï¼Œé€šéå¤šæ¬¡è¿­ä»£ä¾†æé«˜æ¨¡å‹æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a4b9a",
   "metadata": {},
   "source": [
    "## Checkpoint åŠŸèƒ½\n",
    "\n",
    "ç‚ºäº†åœ¨ Colab ä¸­å®‰å…¨è¨“ç·´ä¸¦é¿å…å› ä¸­æ–·è€Œä¸Ÿå¤±é€²åº¦ï¼Œæˆ‘å€‘å¯¦ç¾äº†å®Œæ•´çš„ checkpoint åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a850d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class TrainingCheckpoint:\n",
    "    \"\"\"è¨“ç·´æª¢æŸ¥é»ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir=CHECKPOINT_DIR):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, model, optimizer, iteration, total_iterations, \n",
    "                       training_history, metadata=None):\n",
    "        \"\"\"ä¿å­˜è¨“ç·´æª¢æŸ¥é»\"\"\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_path = self.checkpoint_dir / f\"checkpoint_iter_{iteration}_{timestamp}.pt\"\n",
    "        \n",
    "        checkpoint = {\n",
    "            'iteration': iteration,\n",
    "            'total_iterations': total_iterations,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'training_history': training_history,\n",
    "            'timestamp': timestamp,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # ä¹Ÿä¿å­˜ä¸€å€‹ \"latest\" ç‰ˆæœ¬\n",
    "        latest_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        print(f\"âœ… Checkpoint å·²ä¿å­˜: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path=None, model=None, optimizer=None):\n",
    "        \"\"\"è¼‰å…¥è¨“ç·´æª¢æŸ¥é»\"\"\"\n",
    "        if checkpoint_path is None:\n",
    "            # è¼‰å…¥æœ€æ–°çš„ checkpoint\n",
    "            checkpoint_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "            \n",
    "        if not checkpoint_path.exists():\n",
    "            print(\"âŒ æ²’æœ‰æ‰¾åˆ° checkpoint æª”æ¡ˆ\")\n",
    "            return None\n",
    "            \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        if model is not None:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "        print(f\"âœ… Checkpoint å·²è¼‰å…¥: {checkpoint_path}\")\n",
    "        print(f\"   è¿­ä»£: {checkpoint['iteration']}/{checkpoint['total_iterations']}\")\n",
    "        print(f\"   æ™‚é–“æˆ³: {checkpoint['timestamp']}\")\n",
    "        \n",
    "        return checkpoint\n",
    "    \n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ checkpoint\"\"\"\n",
    "        checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pt\"))\n",
    "        checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        \n",
    "        print(f\"æ‰¾åˆ° {len(checkpoints)} å€‹ checkpoint:\")\n",
    "        for i, cp in enumerate(checkpoints[:10]):  # åªé¡¯ç¤ºæœ€æ–°çš„ 10 å€‹\n",
    "            stat = cp.stat()\n",
    "            size_mb = stat.st_size / (1024 * 1024)\n",
    "            mtime = datetime.datetime.fromtimestamp(stat.st_mtime)\n",
    "            print(f\"  {i+1}. {cp.name} ({size_mb:.1f}MB, {mtime.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "            \n",
    "        return checkpoints\n",
    "    \n",
    "    def clean_old_checkpoints(self, keep_count=5):\n",
    "        \"\"\"æ¸…ç†èˆŠçš„ checkpointï¼Œåªä¿ç•™æœ€æ–°çš„å¹¾å€‹\"\"\"\n",
    "        checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pt\"))\n",
    "        checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        \n",
    "        # ä¿ç•™æœ€æ–°çš„ keep_count å€‹ï¼Œåˆªé™¤å…¶é¤˜çš„\n",
    "        to_delete = checkpoints[keep_count:]\n",
    "        for cp in to_delete:\n",
    "            cp.unlink()\n",
    "            print(f\"ğŸ—‘ï¸  å·²åˆªé™¤èˆŠ checkpoint: {cp.name}\")\n",
    "        \n",
    "        print(f\"âœ… æ¸…ç†å®Œæˆï¼Œä¿ç•™äº† {min(len(checkpoints), keep_count)} å€‹æœ€æ–°çš„ checkpoint\")\n",
    "\n",
    "# åˆå§‹åŒ– checkpoint ç®¡ç†å™¨\n",
    "checkpoint_manager = TrainingCheckpoint()\n",
    "print(\"Checkpoint ç®¡ç†å™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_strong_ai(iterations=10, games_per_iteration=100, training_epochs=5):\n",
    "    \"\"\"è¨“ç·´ä¸€å€‹å¼·å¤§çš„ AI\"\"\"\n",
    "    model = ReversiNet().to(device)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        print(f\"\\né–‹å§‹è¨“ç·´è¿­ä»£ {iteration+1}/{iterations}\")\n",
    "        \n",
    "        # ç”Ÿæˆè‡ªæˆ‘å°å¼ˆæ•¸æ“š\n",
    "        print(\"æ­£åœ¨ç”Ÿæˆè¨“ç·´æ•¸æ“š...\")\n",
    "        states, policies, rewards = generate_training_data(model, num_games=games_per_iteration)\n",
    "        \n",
    "        # è¨“ç·´æ¨¡å‹\n",
    "        print(\"\\nè¨“ç·´æ¨¡å‹...\")\n",
    "        model = train_model(model, states, policies, rewards, epochs=training_epochs)\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        torch.save(model.state_dict(), f'strong_reversi_model_iter_{iteration+1}.pth')\n",
    "        print(f\"æ¨¡å‹å·²ä¿å­˜ç‚º strong_reversi_model_iter_{iteration+1}.pth\")\n",
    "        \n",
    "        # è©•ä¼°æ¨¡å‹\n",
    "        # ï¼ˆæ­¤è™•å¯ä»¥æ·»åŠ å°æŠ—éš¨æ©ŸAIæˆ–è©•ä¼°æ¨¡å‹çš„ä»£ç¢¼ï¼‰\n",
    "        \n",
    "    return model\n",
    "\n",
    "# åŸ·è¡Œå®Œæ•´è¨“ç·´éç¨‹ (æ…é‡åŸ·è¡Œï¼Œè€—æ™‚è¼ƒé•·)\n",
    "# strong_model = train_strong_ai(iterations=5, games_per_iteration=20, training_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_against_random(model, num_games=100):\n",
    "    \"\"\"è©•ä¼°æ¨¡å‹å°æŠ—éš¨æ©ŸAIçš„è¡¨ç¾\"\"\"\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for game_idx in tqdm(range(num_games)):\n",
    "        game = ReversiGame()\n",
    "        model_plays_black = game_idx % 2 == 0  # è¼ªæµåŸ·é»‘åŸ·ç™½\n",
    "        \n",
    "        while not game.game_over:\n",
    "            current_player = game.current_player\n",
    "            valid_moves = game.get_valid_moves()\n",
    "            \n",
    "            if not valid_moves:\n",
    "                # å¦‚æœç•¶å‰ç©å®¶æ²’æœ‰åˆæ³•èµ°æ­¥ï¼Œåˆ‡æ›ç©å®¶\n",
    "                game.current_player = 3 - game.current_player\n",
    "                valid_moves = game.get_valid_moves()\n",
    "                \n",
    "                # å¦‚æœå…©ä½ç©å®¶éƒ½æ²’æœ‰åˆæ³•èµ°æ­¥ï¼ŒéŠæˆ²çµæŸ\n",
    "                if not valid_moves:\n",
    "                    game.game_over = True\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            if (model_plays_black and current_player == 1) or (not model_plays_black and current_player == 2):\n",
    "                # æ¨¡å‹çš„å›åˆ\n",
    "                move = mcts_search(game, model, num_simulations=100)\n",
    "                if move:\n",
    "                    game.make_move(move[0], move[1])\n",
    "            else:\n",
    "                # éš¨æ©ŸAIçš„å›åˆ\n",
    "                move = random.choice(valid_moves)\n",
    "                game.make_move(move[0], move[1])\n",
    "        \n",
    "        # è¨ˆç®—å‹è² \n",
    "        winner = game.get_winner()\n",
    "        if winner == 0:  # å¹³å±€\n",
    "            draws += 1\n",
    "        elif (winner == 1 and model_plays_black) or (winner == 2 and not model_plays_black):\n",
    "            wins += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "    \n",
    "    win_rate = wins / num_games\n",
    "    loss_rate = losses / num_games\n",
    "    draw_rate = draws / num_games\n",
    "    \n",
    "    print(f\"è©•ä¼°çµæœ (å…± {num_games} å ´)\")\n",
    "    print(f\"å‹ç‡: {win_rate:.2%} ({wins} å‹)\")\n",
    "    print(f\"æ•—ç‡: {loss_rate:.2%} ({losses} æ•—)\")\n",
    "    print(f\"å¹³å±€: {draw_rate:.2%} ({draws} å¹³)\")\n",
    "    \n",
    "    return win_rate, loss_rate, draw_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75929254",
   "metadata": {},
   "source": [
    "## å°‡æ¨¡å‹è½‰æ›ç‚º JavaScript æ ¼å¼\n",
    "\n",
    "ç‚ºäº†åœ¨ç€è¦½å™¨ä¸­ä½¿ç”¨æˆ‘å€‘è¨“ç·´çš„æ¨¡å‹ï¼Œæˆ‘å€‘éœ€è¦å°‡å…¶è½‰æ›ç‚º JavaScript æ ¼å¼ã€‚é€™è£¡æˆ‘å€‘å°‡æå–æ¨¡å‹æ¬Šé‡ä¸¦å°‡å…¶å°å‡ºç‚º JSON æ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "def convert_model_to_js(model, filename=\"strong_reversi_model.json\"):\n",
    "    \"\"\"å°‡æ¨¡å‹æ¬Šé‡è½‰æ›ç‚º JavaScript å…¼å®¹çš„ JSON æ ¼å¼\"\"\"\n",
    "    model_weights = {}\n",
    "    \n",
    "    # éæ­·æ¨¡å‹åƒæ•¸\n",
    "    for name, param in model.named_parameters():\n",
    "        # å°‡å¼µé‡è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„åˆ—è¡¨\n",
    "        model_weights[name] = param.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    # ä¿å­˜ç‚º JSON æ–‡ä»¶\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(model_weights, f)\n",
    "    \n",
    "    print(f\"æ¨¡å‹å·²è½‰æ›ä¸¦ä¿å­˜ç‚º {filename}\")\n",
    "    \n",
    "    # æº–å‚™ä¸€å€‹ JavaScript åŒ…è£å™¨ï¼Œä»¥ä¾¿æ–¼åœ¨ç€è¦½å™¨ä¸­ä½¿ç”¨\n",
    "    js_wrapper = f\"\"\"\n",
    "    // é»‘ç™½æ£‹ AI æ¨¡å‹åŒ…è£å™¨\n",
    "    class ReversiAI {{\n",
    "        constructor() {{\n",
    "            // è¼‰å…¥æ¨¡å‹æ¬Šé‡ (é€™å°‡ç”±å¦ä¸€å€‹è…³æœ¬å¡«å……)\n",
    "            this.weights = null;\n",
    "        }}\n",
    "        \n",
    "        // è¼‰å…¥æ¨¡å‹æ¬Šé‡\n",
    "        async loadWeights(url) {{\n",
    "            const response = await fetch(url);\n",
    "            this.weights = await response.json();\n",
    "            console.log('æ¨¡å‹æ¬Šé‡å·²è¼‰å…¥');\n",
    "            return true;\n",
    "        }}\n",
    "        \n",
    "        // ç²å–èµ°æ­¥ (åœ¨é€™è£¡å¯¦ç¾æ¨¡å‹æ¨ç†)\n",
    "        getBestMove(board, currentPlayer) {{\n",
    "            // åœ¨æ­¤è™•å¯¦ç¾æ¨¡å‹æ¨ç†é‚è¼¯\n",
    "            // ...\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    // å°å‡ºä¾›å…¨å±€ä½¿ç”¨\n",
    "    if (typeof module !== 'undefined') {{\n",
    "        module.exports = {{ ReversiAI }};\n",
    "    }} else {{\n",
    "        window.ReversiAI = ReversiAI;\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # ä¿å­˜ JavaScript åŒ…è£å™¨\n",
    "    with open(\"reversi_ai.js\", 'w') as f:\n",
    "        f.write(js_wrapper)\n",
    "    \n",
    "    print(\"JavaScript åŒ…è£å™¨å·²ä¿å­˜ç‚º reversi_ai.js\")\n",
    "    \n",
    "    return model_weights\n",
    "\n",
    "def train_strong_ai_with_checkpoint(iterations=10, games_per_iteration=100, training_epochs=5, \n",
    "                                   resume_from_checkpoint=True, save_every=1):\n",
    "    \"\"\"è¨“ç·´ä¸€å€‹å¼·å¤§çš„ AIï¼Œæ”¯æ´ checkpoint åŠŸèƒ½\"\"\"\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹å’Œå„ªåŒ–å™¨\n",
    "    model = ReversiNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # è¨“ç·´æ­·å²è¨˜éŒ„\n",
    "    training_history = {\n",
    "        'losses': [],\n",
    "        'policy_losses': [],\n",
    "        'value_losses': [],\n",
    "        'win_rates': [],\n",
    "        'iterations_completed': []\n",
    "    }\n",
    "    \n",
    "    start_iteration = 0\n",
    "    \n",
    "    # å˜—è©¦å¾ checkpoint æ¢å¾©\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = checkpoint_manager.load_checkpoint(model=model, optimizer=optimizer)\n",
    "        if checkpoint is not None:\n",
    "            start_iteration = checkpoint['iteration']\n",
    "            training_history = checkpoint['training_history']\n",
    "            print(f\"ğŸ”„ å¾è¿­ä»£ {start_iteration} æ¢å¾©è¨“ç·´\")\n",
    "        else:\n",
    "            print(\"ğŸ†• é–‹å§‹æ–°çš„è¨“ç·´\")\n",
    "    \n",
    "    try:\n",
    "        for iteration in range(start_iteration, iterations):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"é–‹å§‹è¨“ç·´è¿­ä»£ {iteration+1}/{iterations}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # ç”Ÿæˆè‡ªæˆ‘å°å¼ˆæ•¸æ“š\n",
    "            print(\"ğŸ® æ­£åœ¨ç”Ÿæˆè¨“ç·´æ•¸æ“š...\")\n",
    "            states, policies, rewards = generate_training_data(model, num_games=games_per_iteration)\n",
    "            print(f\"ğŸ“Š ç”Ÿæˆäº† {len(states)} å€‹è¨“ç·´æ¨£æœ¬\")\n",
    "            \n",
    "            # è¨“ç·´æ¨¡å‹\n",
    "            print(\"\\nğŸ§  è¨“ç·´æ¨¡å‹...\")\n",
    "            model, epoch_losses = train_model_with_history(model, optimizer, states, policies, rewards, epochs=training_epochs)\n",
    "            \n",
    "            # è¨˜éŒ„è¨“ç·´æ­·å²\n",
    "            training_history['losses'].extend(epoch_losses['total'])\n",
    "            training_history['policy_losses'].extend(epoch_losses['policy'])\n",
    "            training_history['value_losses'].extend(epoch_losses['value'])\n",
    "            training_history['iterations_completed'].append(iteration + 1)\n",
    "            \n",
    "            # è©•ä¼°æ¨¡å‹ï¼ˆæ¯ 2 å€‹è¿­ä»£è©•ä¼°ä¸€æ¬¡ï¼‰\n",
    "            if (iteration + 1) % 2 == 0:\n",
    "                print(\"\\nğŸ“ˆ è©•ä¼°æ¨¡å‹æ€§èƒ½...\")\n",
    "                win_rate, _, _ = evaluate_against_random(model, num_games=20)\n",
    "                training_history['win_rates'].append(win_rate)\n",
    "                print(f\"ç•¶å‰å‹ç‡: {win_rate:.2%}\")\n",
    "            \n",
    "            # ä¿å­˜ checkpoint\n",
    "            if (iteration + 1) % save_every == 0:\n",
    "                metadata = {\n",
    "                    'games_per_iteration': games_per_iteration,\n",
    "                    'training_epochs': training_epochs,\n",
    "                    'current_iteration': iteration + 1,\n",
    "                    'total_iterations': iterations\n",
    "                }\n",
    "                \n",
    "                checkpoint_manager.save_checkpoint(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    iteration=iteration + 1,\n",
    "                    total_iterations=iterations,\n",
    "                    training_history=training_history,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                \n",
    "                # æ¸…ç†èˆŠçš„ checkpointï¼ˆä¿ç•™æœ€æ–°çš„ 5 å€‹ï¼‰\n",
    "                if (iteration + 1) % 3 == 0:\n",
    "                    checkpoint_manager.clean_old_checkpoints(keep_count=5)\n",
    "            \n",
    "            # é¡¯ç¤ºç•¶å‰é€²åº¦\n",
    "            print(f\"\\nâœ… è¿­ä»£ {iteration+1}/{iterations} å®Œæˆ\")\n",
    "            if training_history['losses']:\n",
    "                recent_loss = training_history['losses'][-1]\n",
    "                print(f\"æœ€è¿‘æå¤±: {recent_loss:.4f}\")\n",
    "            \n",
    "            # åœ¨ Colab ä¸­é¡¯ç¤ºé€²åº¦\n",
    "            if IN_COLAB:\n",
    "                progress = (iteration + 1) / iterations * 100\n",
    "                print(f\"ç¸½é€²åº¦: {progress:.1f}%\")\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸  è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·\")\n",
    "        # ä¿å­˜ä¸­æ–·æ™‚çš„ checkpoint\n",
    "        metadata = {\n",
    "            'interrupted': True,\n",
    "            'games_per_iteration': games_per_iteration,\n",
    "            'training_epochs': training_epochs,\n",
    "            'current_iteration': iteration + 1,\n",
    "            'total_iterations': iterations\n",
    "        }\n",
    "        \n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            iteration=iteration + 1,\n",
    "            total_iterations=iterations,\n",
    "            training_history=training_history,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        print(\"ğŸ’¾ ç·Šæ€¥ checkpoint å·²ä¿å­˜\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        # ä¿å­˜éŒ¯èª¤æ™‚çš„ checkpoint\n",
    "        metadata = {\n",
    "            'error': str(e),\n",
    "            'games_per_iteration': games_per_iteration,\n",
    "            'training_epochs': training_epochs,\n",
    "            'current_iteration': iteration + 1,\n",
    "            'total_iterations': iterations\n",
    "        }\n",
    "        \n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            iteration=iteration + 1,\n",
    "            total_iterations=iterations,\n",
    "            training_history=training_history,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        print(\"ğŸ’¾ éŒ¯èª¤ checkpoint å·²ä¿å­˜\")\n",
    "        raise\n",
    "    \n",
    "    # æœ€çµ‚ä¿å­˜\n",
    "    final_metadata = {\n",
    "        'completed': True,\n",
    "        'games_per_iteration': games_per_iteration,\n",
    "        'training_epochs': training_epochs,\n",
    "        'total_iterations': iterations\n",
    "    }\n",
    "    \n",
    "    final_checkpoint_path = checkpoint_manager.save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iteration=iterations,\n",
    "        total_iterations=iterations,\n",
    "        training_history=training_history,\n",
    "        metadata=final_metadata\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ‰ è¨“ç·´å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“ æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: {final_checkpoint_path}\")\n",
    "    \n",
    "    return model, training_history\n",
    "\n",
    "def train_model_with_history(model, optimizer, states, policies, rewards, epochs=5):\n",
    "    \"\"\"æ”¹é€²çš„è¨“ç·´å‡½æ•¸ï¼Œè¿”å›è©³ç´°çš„æå¤±æ­·å²\"\"\"\n",
    "    # å‰µå»ºæ•¸æ“šé›†\n",
    "    X = torch.FloatTensor(states).to(device)\n",
    "    policy_y = torch.FloatTensor(policies).to(device)\n",
    "    value_y = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X, policy_y, value_y)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # æå¤±å‡½æ•¸\n",
    "    policy_criterion = nn.MSELoss()\n",
    "    value_criterion = nn.MSELoss()\n",
    "    \n",
    "    # è¨˜éŒ„æ¯å€‹ epoch çš„æå¤±\n",
    "    epoch_losses = {\n",
    "        'total': [],\n",
    "        'policy': [],\n",
    "        'value': []\n",
    "    }\n",
    "    \n",
    "    # è¨“ç·´å¾ªç’°\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        policy_loss_sum = 0\n",
    "        value_loss_sum = 0\n",
    "        \n",
    "        for batch_x, batch_policy_y, batch_value_y in dataloader:\n",
    "            # å‰å‘å‚³æ’­\n",
    "            policy_pred, value_pred = model(batch_x)\n",
    "            \n",
    "            # è¨ˆç®—æå¤±\n",
    "            policy_loss = policy_criterion(policy_pred, batch_policy_y)\n",
    "            value_loss = value_criterion(value_pred, batch_value_y)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # åå‘å‚³æ’­\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # ç´¯è¨ˆæå¤±\n",
    "            total_loss += loss.item()\n",
    "            policy_loss_sum += policy_loss.item()\n",
    "            value_loss_sum += value_loss.item()\n",
    "        \n",
    "        # è¨ˆç®—å¹³å‡æå¤±\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_policy_loss = policy_loss_sum / len(dataloader)\n",
    "        avg_value_loss = value_loss_sum / len(dataloader)\n",
    "        \n",
    "        # è¨˜éŒ„æå¤±\n",
    "        epoch_losses['total'].append(avg_loss)\n",
    "        epoch_losses['policy'].append(avg_policy_loss)\n",
    "        epoch_losses['value'].append(avg_value_loss)\n",
    "        \n",
    "        # æ‰“å°è¨“ç·´é€²åº¦\n",
    "        print(f'  Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Policy: {avg_policy_loss:.4f}, Value: {avg_value_loss:.4f}')\n",
    "    \n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6ce8b",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "ä»¥ä¸‹æ˜¯è¨“ç·´èˆ‡ä½¿ç”¨å¼· AI æ¨¡å‹çš„å®Œæ•´æµç¨‹ç¤ºä¾‹ã€‚ç”±æ–¼è¨“ç·´éç¨‹éå¸¸è€—æ™‚ï¼Œä»¥ä¸‹ä»£ç¢¼é»˜èªä¸æœƒåŸ·è¡Œå®Œæ•´è¨“ç·´ï¼Œè€Œæ˜¯æä¾›äº†å¿«é€Ÿæ¸¬è©¦å’Œæ¨¡æ“¬è¨“ç·´çš„é¸é …ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c896b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_progress(training_history):\n",
    "    \"\"\"ç¹ªè£½è¨“ç·´é€²åº¦åœ–è¡¨\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('AI è¨“ç·´é€²åº¦', fontsize=16)\n",
    "    \n",
    "    # æå¤±æ›²ç·š\n",
    "    if training_history['losses']:\n",
    "        axes[0, 0].plot(training_history['losses'], label='ç¸½æå¤±', color='red')\n",
    "        axes[0, 0].plot(training_history['policy_losses'], label='ç­–ç•¥æå¤±', color='blue')\n",
    "        axes[0, 0].plot(training_history['value_losses'], label='åƒ¹å€¼æå¤±', color='green')\n",
    "        axes[0, 0].set_title('è¨“ç·´æå¤±')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('æå¤±')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "    \n",
    "    # å‹ç‡æ›²ç·š\n",
    "    if training_history['win_rates']:\n",
    "        axes[0, 1].plot(range(2, len(training_history['win_rates']) * 2 + 1, 2), \n",
    "                       training_history['win_rates'], marker='o', color='purple')\n",
    "        axes[0, 1].set_title('å°æŠ—éš¨æ©Ÿ AI å‹ç‡')\n",
    "        axes[0, 1].set_xlabel('è¿­ä»£')\n",
    "        axes[0, 1].set_ylabel('å‹ç‡')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    # è¿­ä»£å®Œæˆæƒ…æ³\n",
    "    if training_history['iterations_completed']:\n",
    "        axes[1, 0].bar(range(len(training_history['iterations_completed'])), \n",
    "                      training_history['iterations_completed'], color='orange')\n",
    "        axes[1, 0].set_title('å·²å®Œæˆçš„è¿­ä»£')\n",
    "        axes[1, 0].set_xlabel('åºè™Ÿ')\n",
    "        axes[1, 0].set_ylabel('è¿­ä»£æ¬¡æ•¸')\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    # æœ€è¿‘æå¤±è¶¨å‹¢\n",
    "    if len(training_history['losses']) > 10:\n",
    "        recent_losses = training_history['losses'][-50:]  # æœ€è¿‘ 50 å€‹ epoch\n",
    "        axes[1, 1].plot(recent_losses, color='darkred')\n",
    "        axes[1, 1].set_title('æœ€è¿‘æå¤±è¶¨å‹¢')\n",
    "        axes[1, 1].set_xlabel('æœ€è¿‘çš„ Epoch')\n",
    "        axes[1, 1].set_ylabel('æå¤±')\n",
    "        axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_checkpoint_info():\n",
    "    \"\"\"é¡¯ç¤º checkpoint è³‡è¨Š\"\"\"\n",
    "    print(\"ğŸ“‹ Checkpoint ç®¡ç†å™¨ç‹€æ…‹\")\n",
    "    print(f\"ğŸ“ Checkpoint ç›®éŒ„: {checkpoint_manager.checkpoint_dir}\")\n",
    "    \n",
    "    # åˆ—å‡ºå¯ç”¨çš„ checkpoint\n",
    "    checkpoints = checkpoint_manager.list_checkpoints()\n",
    "    \n",
    "    # å¦‚æœæœ‰æœ€æ–°çš„ checkpointï¼Œé¡¯ç¤ºå…¶è©³ç´°è³‡è¨Š\n",
    "    latest_path = checkpoint_manager.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "    if latest_path.exists():\n",
    "        print(\"\\nğŸ“„ æœ€æ–° Checkpoint è©³ç´°è³‡è¨Š:\")\n",
    "        checkpoint = torch.load(latest_path, map_location='cpu')\n",
    "        print(f\"  ğŸ”¢ è¿­ä»£: {checkpoint['iteration']}/{checkpoint['total_iterations']}\")\n",
    "        print(f\"  ğŸ“… æ™‚é–“æˆ³: {checkpoint['timestamp']}\")\n",
    "        \n",
    "        if 'metadata' in checkpoint and checkpoint['metadata']:\n",
    "            print(\"  ğŸ“‹ å…ƒæ•¸æ“š:\")\n",
    "            for key, value in checkpoint['metadata'].items():\n",
    "                print(f\"    {key}: {value}\")\n",
    "        \n",
    "        if 'training_history' in checkpoint:\n",
    "            history = checkpoint['training_history']\n",
    "            if history['losses']:\n",
    "                print(f\"  ğŸ“‰ æœ€æ–°æå¤±: {history['losses'][-1]:.4f}\")\n",
    "            if history['win_rates']:\n",
    "                print(f\"  ğŸ† æœ€æ–°å‹ç‡: {history['win_rates'][-1]:.2%}\")\n",
    "\n",
    "# é¡¯ç¤ºç•¶å‰ checkpoint ç‹€æ…‹\n",
    "show_checkpoint_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638540c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_checkpoint_info():\n",
    "    \"\"\"é¡¯ç¤ºç•¶å‰checkpointç‹€æ…‹ä¿¡æ¯\"\"\"\n",
    "    print(\"ğŸ“Š === Checkpoint ç‹€æ…‹ä¿¡æ¯ ===\")\n",
    "    \n",
    "    # é¡¯ç¤ºcheckpointç›®éŒ„ä¿¡æ¯\n",
    "    print(f\"ğŸ“ Checkpointç›®éŒ„: {checkpoint_manager.checkpoint_dir}\")\n",
    "    \n",
    "    # ç²å–æœ€æ–°checkpointä¿¡æ¯\n",
    "    info = checkpoint_manager.get_checkpoint_info()\n",
    "    \n",
    "    if info is None:\n",
    "        print(\"ğŸ“­ ç›®å‰æ²’æœ‰å¯ç”¨çš„checkpoint\")\n",
    "        print(\"ğŸš€ ä¸‹æ¬¡è¨“ç·´å°‡å¾é ­é–‹å§‹\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ“ˆ ç•¶å‰é€²åº¦: {info['iteration']}/{info['total_iterations']} è¿­ä»£\")\n",
    "    print(f\"ğŸ“… æœ€å¾Œæ›´æ–°: {info['timestamp']}\")\n",
    "    \n",
    "    # é¡¯ç¤ºè¨“ç·´æ­·å²çµ±è¨ˆ\n",
    "    history = info['training_history']\n",
    "    if history.get('losses'):\n",
    "        print(f\"ğŸ“‰ æœ€æ–°æå¤±: {history['losses'][-1]:.4f}\")\n",
    "        print(f\"ğŸ“Š ç¸½è¨“ç·´epoch: {len(history['losses'])}\")\n",
    "    \n",
    "    if history.get('win_rates'):\n",
    "        print(f\"ğŸ† æœ€æ–°å‹ç‡: {history['win_rates'][-1]:.2%}\")\n",
    "        print(f\"ğŸ“ˆ è©•ä¼°æ¬¡æ•¸: {len(history['win_rates'])}\")\n",
    "    \n",
    "    # é¡¯ç¤ºå…ƒæ•¸æ“š\n",
    "    metadata = info.get('metadata', {})\n",
    "    if metadata:\n",
    "        print(\"ğŸ“‹ é¡å¤–ä¿¡æ¯:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # é¡¯ç¤ºå¯ç”¨checkpointåˆ—è¡¨\n",
    "    checkpoint_files = checkpoint_manager.list_checkpoints()\n",
    "    print(f\"ğŸ’¾ å¯ç”¨checkpointæ•¸é‡: {len(checkpoint_files)}\")\n",
    "    \n",
    "    if checkpoint_files:\n",
    "        print(\"ğŸ“‹ æœ€è¿‘çš„checkpoint:\")\n",
    "        for i, cp_file in enumerate(checkpoint_files[:3]):\n",
    "            size_mb = cp_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   {i+1}. {cp_file.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_strong_ai_with_checkpoint(iterations=10, games_per_iteration=100, \n",
    "                                   training_epochs=5, resume_from_checkpoint=True, \n",
    "                                   save_every=2, fast_test=False):\n",
    "    \"\"\"ä½¿ç”¨checkpointåŠŸèƒ½è¨“ç·´å¼·åŠ›AI\"\"\"\n",
    "    print(f\"ğŸš€ é–‹å§‹è¨“ç·´å¼·åŠ›AIï¼ˆ{'å¿«é€Ÿæ¸¬è©¦' if fast_test else 'å®Œæ•´è¨“ç·´'}ï¼‰\")\n",
    "    print(f\"ğŸ“Š è¨“ç·´åƒæ•¸: {iterations}è¿­ä»£ x {games_per_iteration}å ´éŠæˆ² x {training_epochs}å€‹epoch\")\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹å’Œå„ªåŒ–å™¨\n",
    "    model = ReversiNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # åˆå§‹åŒ–è¨“ç·´æ­·å²\n",
    "    training_history = {\n",
    "        'losses': [],\n",
    "        'policy_losses': [],\n",
    "        'value_losses': [],\n",
    "        'win_rates': [],\n",
    "        'iterations': []\n",
    "    }\n",
    "    \n",
    "    start_iteration = 0\n",
    "    \n",
    "    # å˜—è©¦è¼‰å…¥checkpoint\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = checkpoint_manager.load_latest_checkpoint(model, optimizer)\n",
    "        if checkpoint:\n",
    "            start_iteration = checkpoint['iteration']\n",
    "            training_history = checkpoint.get('training_history', training_history)\n",
    "            print(f\"ğŸ“¥ å¾è¿­ä»£ {start_iteration} ç¹¼çºŒè¨“ç·´\")\n",
    "    \n",
    "    # å¦‚æœå¿«é€Ÿæ¸¬è©¦ï¼Œèª¿æ•´åƒæ•¸\n",
    "    if fast_test:\n",
    "        iterations = min(iterations, 2)\n",
    "        games_per_iteration = min(games_per_iteration, 5)\n",
    "        training_epochs = min(training_epochs, 1)\n",
    "        save_every = 1\n",
    "        print(f\"ğŸ§ª å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼š{iterations}è¿­ä»£ x {games_per_iteration}å ´éŠæˆ² x {training_epochs}å€‹epoch\")\n",
    "    \n",
    "    try:\n",
    "        for iteration in range(start_iteration, iterations):\n",
    "            print(f\"\\nğŸ”„ === è¿­ä»£ {iteration+1}/{iterations} ===\")\n",
    "            \n",
    "            # æ”¶é›†è¨“ç·´æ•¸æ“š\n",
    "            print(\"ğŸ® æ”¶é›†è‡ªæˆ‘å°å¼ˆæ•¸æ“š...\")\n",
    "            states, policies, rewards = collect_selfplay_data(\n",
    "                model, num_games=games_per_iteration, \n",
    "                use_mcts=(not fast_test), exploration_weight=1.0\n",
    "            )\n",
    "            \n",
    "            print(f\"ğŸ“š æ”¶é›†åˆ° {len(states)} å€‹è¨“ç·´æ¨£æœ¬\")\n",
    "            \n",
    "            # è¨“ç·´æ¨¡å‹\n",
    "            print(f\"ğŸ‹ï¸â€â™‚ï¸ é–‹å§‹ç¥ç¶“ç¶²è·¯è¨“ç·´ï¼ˆ{training_epochs} epochsï¼‰...\")\n",
    "            model, epoch_losses = train_model_with_history(\n",
    "                model, optimizer, states, policies, rewards, epochs=training_epochs\n",
    "            )\n",
    "            \n",
    "            # è¨˜éŒ„è¨“ç·´æ­·å²\n",
    "            if epoch_losses['total']:\n",
    "                training_history['losses'].extend(epoch_losses['total'])\n",
    "                training_history['policy_losses'].extend(epoch_losses['policy'])\n",
    "                training_history['value_losses'].extend(epoch_losses['value'])\n",
    "                training_history['iterations'].extend([iteration+1] * len(epoch_losses['total']))\n",
    "            \n",
    "            # è©•ä¼°æ¨¡å‹ï¼ˆæ¯2å€‹è¿­ä»£æˆ–åœ¨å¿«é€Ÿæ¸¬è©¦ä¸­æ¯æ¬¡éƒ½è©•ä¼°ï¼‰\n",
    "            if (iteration + 1) % 2 == 0 or fast_test:\n",
    "                print(\"\\nğŸ“ˆ è©•ä¼°æ¨¡å‹æ€§èƒ½...\")\n",
    "                num_eval_games = 10 if fast_test else 50\n",
    "                win_rate, _, _ = evaluate_against_random(model, num_games=num_eval_games)\n",
    "                training_history['win_rates'].append(win_rate)\n",
    "                print(f\"ğŸ¯ ç•¶å‰å‹ç‡: {win_rate:.2%}\")\n",
    "                \n",
    "                # å¦‚æœå‹ç‡å¾ˆé«˜ï¼Œå¯ä»¥æå‰çµæŸå¿«é€Ÿæ¸¬è©¦\n",
    "                if fast_test and win_rate > 0.9:\n",
    "                    print(\"âœ… å¿«é€Ÿæ¸¬è©¦ï¼šå‹ç‡å·²é”90%ï¼Œæ¸¬è©¦æˆåŠŸï¼\")\n",
    "                    break\n",
    "            \n",
    "            # ä¿å­˜checkpoint\n",
    "            if (iteration + 1) % save_every == 0:\n",
    "                metadata = {\n",
    "                    'games_per_iteration': games_per_iteration,\n",
    "                    'training_epochs': training_epochs,\n",
    "                    'current_iteration': iteration + 1,\n",
    "                    'total_iterations': iterations,\n",
    "                    'fast_test': fast_test\n",
    "                }\n",
    "                \n",
    "                checkpoint_manager.save_checkpoint(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    iteration=iteration + 1,\n",
    "                    total_iterations=iterations,\n",
    "                    training_history=training_history,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "            \n",
    "            # å®šæœŸæ¸…ç†èˆŠcheckpoint\n",
    "            if (iteration + 1) % 3 == 0:\n",
    "                checkpoint_manager.clean_old_checkpoints(keep_count=5)\n",
    "            \n",
    "            # é¡¯ç¤ºé€²åº¦\n",
    "            progress = (iteration + 1) / iterations * 100\n",
    "            print(f\"\\nâœ… è¿­ä»£ {iteration+1}/{iterations} å®Œæˆ ({progress:.1f}%)\")\n",
    "            if training_history['losses']:\n",
    "                recent_loss = training_history['losses'][-1]\n",
    "                print(f\"ğŸ“‰ æœ€è¿‘æå¤±: {recent_loss:.4f}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸  è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·\")\n",
    "        # ä¿å­˜ä¸­æ–·checkpoint\n",
    "        metadata = {\n",
    "            'interrupted': True,\n",
    "            'games_per_iteration': games_per_iteration,\n",
    "            'training_epochs': training_epochs,\n",
    "            'current_iteration': iteration + 1,\n",
    "            'total_iterations': iterations,\n",
    "            'fast_test': fast_test\n",
    "        }\n",
    "        \n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            iteration=iteration + 1,\n",
    "            total_iterations=iterations,\n",
    "            training_history=training_history,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        print(\"ğŸ’¾ ç·Šæ€¥checkpointå·²ä¿å­˜\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        # ä¿å­˜éŒ¯èª¤checkpoint\n",
    "        metadata = {\n",
    "            'error': str(e),\n",
    "            'games_per_iteration': games_per_iteration,\n",
    "            'training_epochs': training_epochs,\n",
    "            'current_iteration': iteration + 1,\n",
    "            'total_iterations': iterations,\n",
    "            'fast_test': fast_test\n",
    "        }\n",
    "        \n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            iteration=iteration + 1,\n",
    "            total_iterations=iterations,\n",
    "            training_history=training_history,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        print(\"ğŸ’¾ éŒ¯èª¤checkpointå·²ä¿å­˜\")\n",
    "        raise\n",
    "    \n",
    "    # ä¿å­˜æœ€çµ‚checkpoint\n",
    "    final_metadata = {\n",
    "        'completed': True,\n",
    "        'games_per_iteration': games_per_iteration,\n",
    "        'training_epochs': training_epochs,\n",
    "        'total_iterations': iterations,\n",
    "        'fast_test': fast_test\n",
    "    }\n",
    "    \n",
    "    final_checkpoint_path = checkpoint_manager.save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iteration=iterations,\n",
    "        total_iterations=iterations,\n",
    "        training_history=training_history,\n",
    "        metadata=final_metadata\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ‰ è¨“ç·´å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“ æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: {final_checkpoint_path}\")\n",
    "    \n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­ç½®è¨“ç·´åƒæ•¸\n",
    "fast_test = True  # æ”¹ç‚º False ä»¥åŸ·è¡Œå®Œæ•´è¨“ç·´\n",
    "\n",
    "if fast_test:\n",
    "    # å¿«é€Ÿæ¸¬è©¦ - è¨“ç·´æ¥µå°ç‰ˆæœ¬ä»¥é©—è­‰ä»£ç¢¼å¯ç”¨\n",
    "    print(\"åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦...\")\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    test_model = ReversiNet().to(device)\n",
    "    \n",
    "    # ç”Ÿæˆå°‘é‡è¨“ç·´æ•¸æ“š\n",
    "    print(\"ç”Ÿæˆå°‘é‡è¨“ç·´æ•¸æ“š...\")\n",
    "    states, policies, rewards = generate_training_data(test_model, num_games=2)\n",
    "    \n",
    "    # æ¸¬è©¦è¨“ç·´\n",
    "    print(\"\\næ¸¬è©¦è¨“ç·´æ¨¡å‹...\")\n",
    "    test_model = train_model(test_model, states, policies, rewards, epochs=1)\n",
    "    \n",
    "    # æ¸¬è©¦è½‰æ›ç‚º JS æ ¼å¼\n",
    "    print(\"\\næ¸¬è©¦è½‰æ›ç‚º JavaScript æ ¼å¼...\")\n",
    "    convert_model_to_js(test_model, \"test_reversi_model.json\")\n",
    "    \n",
    "    print(\"\\nå¿«é€Ÿæ¸¬è©¦å®Œæˆã€‚è‹¥è¦åŸ·è¡Œå®Œæ•´è¨“ç·´ï¼Œè«‹å°‡ fast_test è¨­ç½®ç‚º False\")\n",
    "else:\n",
    "    # å®Œæ•´è¨“ç·´éç¨‹\n",
    "    print(\"é–‹å§‹å®Œæ•´è¨“ç·´éç¨‹...\")\n",
    "    \n",
    "    # è¨“ç·´å¼·åŠ› AI\n",
    "    strong_model = train_strong_ai(iterations=10, games_per_iteration=100, training_epochs=5)\n",
    "    \n",
    "    # è©•ä¼°æ¨¡å‹\n",
    "    print(\"\\nè©•ä¼°æ¨¡å‹å°æŠ—éš¨æ©Ÿ AI...\")\n",
    "    evaluate_against_random(strong_model, num_games=50)\n",
    "    \n",
    "    # è½‰æ›ç‚º JavaScript æ ¼å¼\n",
    "    print(\"\\nå°‡æ¨¡å‹è½‰æ›ç‚º JavaScript æ ¼å¼...\")\n",
    "    convert_model_to_js(strong_model, \"strong_reversi_model.json\")\n",
    "    \n",
    "    print(\"\\nè¨“ç·´å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ä¸¦è½‰æ›ç‚º JavaScript æ ¼å¼ã€‚\")\n",
    "\n",
    "def plot_training_progress(training_history):\n",
    "    \"\"\"ç¹ªè£½è©³ç´°çš„è¨“ç·´é€²åº¦åœ–è¡¨\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('ğŸ¤– AI è¨“ç·´é€²åº¦ç›£æ§', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # æå¤±æ›²ç·š\n",
    "    if training_history.get('losses'):\n",
    "        axes[0, 0].plot(training_history['losses'], label='ğŸ“‰ ç¸½æå¤±', color='red', linewidth=2)\n",
    "        \n",
    "        if training_history.get('policy_losses'):\n",
    "            axes[0, 0].plot(training_history['policy_losses'], label='ğŸ¯ ç­–ç•¥æå¤±', \n",
    "                           color='blue', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "        if training_history.get('value_losses'):\n",
    "            axes[0, 0].plot(training_history['value_losses'], label='ğŸ’ åƒ¹å€¼æå¤±', \n",
    "                           color='green', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "        axes[0, 0].set_title('ğŸ“ˆ è¨“ç·´æå¤±è®ŠåŒ–', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('æå¤±å€¼')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_yscale('log')  # ä½¿ç”¨å°æ•¸å°ºåº¦æ›´å¥½è§€å¯Ÿæå¤±è®ŠåŒ–\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'æš«ç„¡æå¤±æ•¸æ“š', ha='center', va='center', fontsize=12)\n",
    "        axes[0, 0].set_title('ğŸ“ˆ è¨“ç·´æå¤±è®ŠåŒ–')\n",
    "    \n",
    "    # å‹ç‡æ›²ç·š\n",
    "    if training_history.get('win_rates'):\n",
    "        evaluation_points = list(range(2, len(training_history['win_rates']) * 2 + 1, 2))\n",
    "        axes[0, 1].plot(evaluation_points, training_history['win_rates'], \n",
    "                       marker='o', markersize=8, color='purple', linewidth=2, \n",
    "                       markerfacecolor='gold', markeredgecolor='purple')\n",
    "        \n",
    "        # æ·»åŠ ç›®æ¨™ç·š\n",
    "        axes[0, 1].axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='ğŸ¯ ç›®æ¨™å‹ç‡ (90%)')\n",
    "        axes[0, 1].axhline(y=0.7, color='orange', linestyle='--', alpha=0.7, label='ğŸ“Š è‰¯å¥½å‹ç‡ (70%)')\n",
    "        \n",
    "        axes[0, 1].set_title('ğŸ† å°æŠ—éš¨æ©ŸAIå‹ç‡', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('è¿­ä»£æ¬¡æ•¸')\n",
    "        axes[0, 1].set_ylabel('å‹ç‡')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "        for i, (x, y) in enumerate(zip(evaluation_points, training_history['win_rates'])):\n",
    "            axes[0, 1].annotate(f'{y:.2%}', (x, y), textcoords=\"offset points\", \n",
    "                               xytext=(0,10), ha='center', fontsize=9)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'æš«ç„¡å‹ç‡æ•¸æ“š', ha='center', va='center', fontsize=12)\n",
    "        axes[0, 1].set_title('ğŸ† å°æŠ—éš¨æ©ŸAIå‹ç‡')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # è¨“ç·´é€²åº¦çµ±è¨ˆ\n",
    "    axes[1, 0].axis('off')\n",
    "    stats_text = \"ğŸ“Š è¨“ç·´çµ±è¨ˆ:\\n\\n\"\n",
    "    \n",
    "    if training_history.get('losses'):\n",
    "        stats_text += f\"ğŸ“‰ ç¸½è¨“ç·´epochs: {len(training_history['losses'])}\\n\"\n",
    "        stats_text += f\"ğŸ”» æœ€ä½æå¤±: {min(training_history['losses']):.4f}\\n\"\n",
    "        stats_text += f\"ğŸ“ˆ æœ€æ–°æå¤±: {training_history['losses'][-1]:.4f}\\n\\n\"\n",
    "    \n",
    "    if training_history.get('win_rates'):\n",
    "        stats_text += f\"ğŸ† æœ€é«˜å‹ç‡: {max(training_history['win_rates']):.2%}\\n\"\n",
    "        stats_text += f\"ğŸ“Š æœ€æ–°å‹ç‡: {training_history['win_rates'][-1]:.2%}\\n\"\n",
    "        stats_text += f\"ğŸ“ˆ è©•ä¼°æ¬¡æ•¸: {len(training_history['win_rates'])}\\n\\n\"\n",
    "    \n",
    "    # æ·»åŠ checkpointä¿¡æ¯\n",
    "    checkpoint_info = checkpoint_manager.get_checkpoint_info()\n",
    "    if checkpoint_info:\n",
    "        stats_text += f\"ğŸ’¾ Checkpointç‹€æ…‹:\\n\"\n",
    "        stats_text += f\"   è¿­ä»£: {checkpoint_info['iteration']}/{checkpoint_info['total_iterations']}\\n\"\n",
    "        stats_text += f\"   æ›´æ–°æ™‚é–“: {checkpoint_info['timestamp']}\\n\"\n",
    "    \n",
    "    axes[1, 0].text(0.05, 0.95, stats_text, transform=axes[1, 0].transAxes, \n",
    "                    fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    # æå¤±åˆ†ä½ˆç›´æ–¹åœ–ï¼ˆå¦‚æœæœ‰è¶³å¤ æ•¸æ“šï¼‰\n",
    "    if training_history.get('losses') and len(training_history['losses']) > 10:\n",
    "        recent_losses = training_history['losses'][-50:]  # æœ€è¿‘50å€‹epochçš„æå¤±\n",
    "        axes[1, 1].hist(recent_losses, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1, 1].axvline(np.mean(recent_losses), color='red', linestyle='--', \n",
    "                          label=f'å¹³å‡å€¼: {np.mean(recent_losses):.4f}')\n",
    "        axes[1, 1].set_title('ğŸ“Š æœ€è¿‘æå¤±åˆ†ä½ˆ', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('æå¤±å€¼')\n",
    "        axes[1, 1].set_ylabel('é »æ¬¡')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'éœ€è¦æ›´å¤šæ•¸æ“š\\næ‰èƒ½é¡¯ç¤ºåˆ†ä½ˆ', ha='center', va='center', fontsize=12)\n",
    "        axes[1, 1].set_title('ğŸ“Š æœ€è¿‘æå¤±åˆ†ä½ˆ')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # é¡¯ç¤ºè©³ç´°çš„é€²åº¦å ±å‘Š\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“Š è©³ç´°è¨“ç·´å ±å‘Š\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if training_history.get('losses'):\n",
    "        print(f\"ğŸ“‰ æå¤±è¶¨å‹¢: {training_history['losses'][0]:.4f} â†’ {training_history['losses'][-1]:.4f}\")\n",
    "        \n",
    "        # è¨ˆç®—æ”¹å–„ç‡\n",
    "        if len(training_history['losses']) > 1:\n",
    "            improvement = (training_history['losses'][0] - training_history['losses'][-1]) / training_history['losses'][0] * 100\n",
    "            print(f\"ğŸ“ˆ æå¤±æ”¹å–„: {improvement:.1f}%\")\n",
    "    \n",
    "    if training_history.get('win_rates'):\n",
    "        print(f\"ğŸ† å‹ç‡è¶¨å‹¢: {training_history['win_rates'][0]:.2%} â†’ {training_history['win_rates'][-1]:.2%}\")\n",
    "        \n",
    "        # å‹ç‡ç©©å®šæ€§\n",
    "        if len(training_history['win_rates']) > 3:\n",
    "            recent_std = np.std(training_history['win_rates'][-3:])\n",
    "            print(f\"ğŸ“Š æœ€è¿‘å‹ç‡ç©©å®šæ€§: Â±{recent_std:.2%}\")\n",
    "    \n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6c3d9",
   "metadata": {},
   "source": [
    "## çµè«–èˆ‡ä½¿ç”¨èªªæ˜\n",
    "\n",
    "æˆ‘å€‘å·²ç¶“æˆåŠŸå¯¦ç¾äº†ä¸€å€‹åŸºæ–¼æ·±åº¦å­¸ç¿’çš„é»‘ç™½æ£‹ AI æ¨¡å‹ï¼Œå®ƒé€šéè‡ªæˆ‘å°å¼ˆå’Œå¼·åŒ–å­¸ç¿’ä¾†æå‡æ£‹åŠ›ï¼Œä½¿å…¶éå¸¸ä¸å®¹æ˜“è¼¸ã€‚\n",
    "\n",
    "### å¦‚ä½•ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹\n",
    "\n",
    "1. åŸ·è¡Œå®Œæ•´è¨“ç·´éç¨‹ï¼ˆå°‡ `fast_test` è¨­ç½®ç‚º `False`ï¼‰\n",
    "2. è¨“ç·´å®Œæˆå¾Œï¼Œä½ å°‡å¾—åˆ°å…©å€‹æ–‡ä»¶ï¼š\n",
    "   - `strong_reversi_model.json`ï¼šåŒ…å«æ¨¡å‹æ¬Šé‡çš„ JSON æ–‡ä»¶\n",
    "   - `reversi_ai.js`ï¼šJavaScript åŒ…è£å™¨ï¼Œç”¨æ–¼åœ¨ç€è¦½å™¨ä¸­åŠ è¼‰å’Œä½¿ç”¨æ¨¡å‹\n",
    "\n",
    "3. åœ¨ä½ çš„ Web æ‡‰ç”¨ä¸­ä½¿ç”¨ï¼š\n",
    "   ```javascript\n",
    "   // åˆå§‹åŒ– AI\n",
    "   const ai = new ReversiAI();\n",
    "   \n",
    "   // è¼‰å…¥æ¨¡å‹æ¬Šé‡\n",
    "   await ai.loadWeights('strong_reversi_model.json');\n",
    "   \n",
    "   // ç²å–æœ€ä½³èµ°æ­¥\n",
    "   const bestMove = ai.getBestMove(boardState, currentPlayer);\n",
    "   ```\n",
    "\n",
    "### é€²ä¸€æ­¥æ”¹é€²\n",
    "\n",
    "- å¢åŠ è¨“ç·´è¿­ä»£æ¬¡æ•¸å’Œè‡ªæˆ‘å°å¼ˆçš„éŠæˆ²æ•¸é‡\n",
    "- èª¿æ•´ç¥ç¶“ç¶²è·¯æ¶æ§‹ï¼Œå¢åŠ ç¶²è·¯çš„æ·±åº¦å’Œå¯¬åº¦\n",
    "- å¯¦ç¾æ›´è¤‡é›œçš„æœç´¢ç®—æ³•ï¼Œå¦‚ AlphaZero ä¸­çš„ PUCT ç®—æ³•\n",
    "- æ·»åŠ æ£‹ç›¤ç‹€æ…‹å¢å¼·ï¼Œé€šéæ—‹è½‰å’Œé¡åƒä¾†å¢åŠ è¨“ç·´æ•¸æ“šçš„å¤šæ¨£æ€§\n",
    "\n",
    "## è¨“ç·´éç¨‹è©³è§£\n",
    "\n",
    "### è¨­ç½®è¨“ç·´åƒæ•¸\n",
    "\n",
    "```python\n",
    "fast_test = True  # æ”¹ç‚º False ä»¥åŸ·è¡Œå®Œæ•´è¨“ç·´\n",
    "resume_training = True  # æ˜¯å¦å¾ checkpoint æ¢å¾©è¨“ç·´\n",
    "```\n",
    "\n",
    "### å¿«é€Ÿæ¸¬è©¦\n",
    "\n",
    "ç•¶ `fast_test` è¨­ç½®ç‚º `True` æ™‚ï¼Œç³»çµ±å°‡åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦ï¼Œé€™æ˜¯ä¸€å€‹è¨“ç·´æ¥µå°ç‰ˆæœ¬çš„éç¨‹ï¼Œä¸»è¦ç”¨æ–¼é©—è­‰ä»£ç¢¼çš„å¯ç”¨æ€§ã€‚\n",
    "\n",
    "- åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦çš„æ­¥é©Ÿï¼š\n",
    "  1. é¡¯ç¤ºç•¶å‰çš„ checkpoint ç‹€æ…‹\n",
    "  2. ä½¿ç”¨æ¸¬è©¦åƒæ•¸é€²è¡Œè¨“ç·´\n",
    "  3. é¡¯ç¤ºè¨“ç·´é€²åº¦\n",
    "  4. æ¸¬è©¦æ¨¡å‹è½‰æ›ç‚º JavaScript æ ¼å¼\n",
    "\n",
    "### å®Œæ•´è¨“ç·´éç¨‹\n",
    "\n",
    "ç•¶ `fast_test` è¨­ç½®ç‚º `False` æ™‚ï¼Œç³»çµ±å°‡åŸ·è¡Œå®Œæ•´çš„è¨“ç·´éç¨‹ã€‚\n",
    "\n",
    "- å®Œæ•´è¨“ç·´éç¨‹åŒ…æ‹¬ï¼š\n",
    "  1. é¡¯ç¤ºç•¶å‰çš„ checkpoint ç‹€æ…‹\n",
    "  2. è¨“ç·´å¼·åŠ› AIï¼Œæ”¯æ´å¾ checkpoint æ¢å¾©è¨“ç·´\n",
    "  3. é¡¯ç¤ºè¨“ç·´é€²åº¦\n",
    "  4. æœ€çµ‚è©•ä¼°æ¨¡å‹\n",
    "  5. å°‡æ¨¡å‹è½‰æ›ç‚º JavaScript æ ¼å¼\n",
    "  6. ä¿å­˜æœ€çµ‚è¨“ç·´æ­·å²\n",
    "  7. æ¸…ç†èˆŠçš„ checkpoint\n",
    "\n",
    "### ä»£ç¢¼ç¯„ä¾‹\n",
    "\n",
    "ä»¥ä¸‹æ˜¯è¨“ç·´éç¨‹çš„ä¸»è¦ä»£ç¢¼ç¯„ä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "if fast_test:\n",
    "    # å¿«é€Ÿæ¸¬è©¦ - è¨“ç·´æ¥µå°ç‰ˆæœ¬ä»¥é©—è­‰ä»£ç¢¼å¯ç”¨\n",
    "    print(\"ğŸš€ åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦...\")\n",
    "    \n",
    "    # é¡¯ç¤º checkpoint ç‹€æ…‹\n",
    "    show_checkpoint_info()\n",
    "    \n",
    "    # ä½¿ç”¨ checkpoint åŠŸèƒ½é€²è¡Œæ¸¬è©¦è¨“ç·´\n",
    "    print(\"\\nğŸ§ª é–‹å§‹æ¸¬è©¦è¨“ç·´ï¼ˆåŒ…å« checkpoint åŠŸèƒ½ï¼‰...\")\n",
    "    test_model, test_history = train_strong_ai_with_checkpoint(\n",
    "        iterations=2,  # åªåŸ·è¡Œ 2 å€‹è¿­ä»£\n",
    "        games_per_iteration=5,  # æ¯æ¬¡è¿­ä»£åªç© 5 å ´éŠæˆ²\n",
    "        training_epochs=1,  # æ¯æ¬¡åªè¨“ç·´ 1 å€‹ epoch\n",
    "        resume_from_checkpoint=resume_training,\n",
    "        save_every=1  # æ¯å€‹è¿­ä»£éƒ½ä¿å­˜\n",
    "    )\n",
    "    \n",
    "    # é¡¯ç¤ºè¨“ç·´é€²åº¦\n",
    "    if test_history['losses']:\n",
    "        plot_training_progress(test_history)\n",
    "    \n",
    "    # æ¸¬è©¦è½‰æ›ç‚º JS æ ¼å¼\n",
    "    print(\"\\nğŸ”„ æ¸¬è©¦è½‰æ›ç‚º JavaScript æ ¼å¼...\")\n",
    "    convert_model_to_js(test_model, \"test_reversi_model.json\")\n",
    "    \n",
    "    print(\"\\nâœ… å¿«é€Ÿæ¸¬è©¦å®Œæˆã€‚è‹¥è¦åŸ·è¡Œå®Œæ•´è¨“ç·´ï¼Œè«‹å°‡ fast_test è¨­ç½®ç‚º False\")\n",
    "    print(\"ğŸ’¡ æç¤º: å¦‚æœä¸­æ–·è¨“ç·´ï¼Œä¸‹æ¬¡é‹è¡Œæ™‚æœƒè‡ªå‹•å¾ checkpoint æ¢å¾©\")\n",
    "    \n",
    "else:\n",
    "    # å®Œæ•´è¨“ç·´éç¨‹\n",
    "    print(\"ğŸ‹ï¸â€â™‚ï¸ é–‹å§‹å®Œæ•´è¨“ç·´éç¨‹...\")\n",
    "    \n",
    "    # é¡¯ç¤ºç•¶å‰ checkpoint ç‹€æ…‹\n",
    "    show_checkpoint_info()\n",
    "    \n",
    "    # è¨“ç·´å¼·åŠ› AIï¼ˆæ”¯æ´ checkpointï¼‰\n",
    "    print(\"\\nğŸ¤– é–‹å§‹è¨“ç·´å¼·åŠ› AI...\")\n",
    "    strong_model, training_history = train_strong_ai_with_checkpoint(\n",
    "        iterations=15,  # å¢åŠ è¿­ä»£æ¬¡æ•¸\n",
    "        games_per_iteration=100,  # æ¯æ¬¡è¿­ä»£ 100 å ´éŠæˆ²\n",
    "        training_epochs=5,  # æ¯æ¬¡è¨“ç·´ 5 å€‹ epoch\n",
    "        resume_from_checkpoint=resume_training,\n",
    "        save_every=2  # æ¯ 2 å€‹è¿­ä»£ä¿å­˜ä¸€æ¬¡\n",
    "    )\n",
    "    \n",
    "    # é¡¯ç¤ºå®Œæ•´çš„è¨“ç·´é€²åº¦\n",
    "    print(\"\\nğŸ“Š é¡¯ç¤ºè¨“ç·´é€²åº¦...\")\n",
    "    plot_training_progress(training_history)\n",
    "    \n",
    "    # æœ€çµ‚è©•ä¼°æ¨¡å‹\n",
    "    print(\"\\nğŸ† æœ€çµ‚è©•ä¼°æ¨¡å‹å°æŠ—éš¨æ©Ÿ AI...\")\n",
    "    final_win_rate, _, _ = evaluate_against_random(strong_model, num_games=100)\n",
    "    print(f\"ğŸ¯ æœ€çµ‚å‹ç‡: {final_win_rate:.2%}\")\n",
    "    \n",
    "    # è½‰æ›ç‚º JavaScript æ ¼å¼\n",
    "    print(\"\\nğŸ”„ å°‡æ¨¡å‹è½‰æ›ç‚º JavaScript æ ¼å¼...\")\n",
    "    convert_model_to_js(strong_model, \"strong_reversi_model.json\")\n",
    "    \n",
    "    # ä¿å­˜æœ€çµ‚è¨“ç·´æ­·å²\n",
    "    with open(checkpoint_manager.checkpoint_dir / \"final_training_history.pkl\", 'wb') as f:\n",
    "        pickle.dump(training_history, f)\n",
    "    \n",
    "    print(\"\\nğŸ‰ è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ä¸¦è½‰æ›ç‚º JavaScript æ ¼å¼ã€‚\")\n",
    "    print(f\"ğŸ“ æ‰€æœ‰æª”æ¡ˆå·²ä¿å­˜åˆ°: {checkpoint_manager.checkpoint_dir}\")\n",
    "    \n",
    "    # æœ€å¾Œæ¸…ç†èˆŠçš„ checkpoint\n",
    "    checkpoint_manager.clean_old_checkpoints(keep_count=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61003f05",
   "metadata": {},
   "source": [
    "## çµè«–èˆ‡ä½¿ç”¨èªªæ˜\n",
    "\n",
    "æˆ‘å€‘å·²ç¶“æˆåŠŸå¯¦ç¾äº†ä¸€å€‹åŸºæ–¼æ·±åº¦å­¸ç¿’çš„é»‘ç™½æ£‹ AI æ¨¡å‹ï¼Œå…·å‚™å®Œæ•´çš„ checkpoint åŠŸèƒ½ï¼Œè®“è¨“ç·´éç¨‹æ›´åŠ å®‰å…¨å¯é ã€‚\n",
    "\n",
    "### ğŸ”„ Checkpoint åŠŸèƒ½ç‰¹é»\n",
    "\n",
    "1. **è‡ªå‹•ä¿å­˜**: è¨“ç·´éç¨‹ä¸­å®šæœŸè‡ªå‹•ä¿å­˜é€²åº¦\n",
    "2. **æ–·é»æ¢å¾©**: å¦‚æœè¨“ç·´ä¸­æ–·ï¼Œå¯ä»¥å¾æœ€å¾Œä¸€å€‹ checkpoint æ¢å¾©\n",
    "3. **é€²åº¦è¿½è¹¤**: è©³ç´°è¨˜éŒ„è¨“ç·´æ­·å²å’Œæ€§èƒ½æŒ‡æ¨™\n",
    "4. **éŒ¯èª¤æ¢å¾©**: å³ä½¿ç™¼ç”ŸéŒ¯èª¤ä¹Ÿæœƒä¿å­˜ç•¶å‰ç‹€æ…‹\n",
    "5. **ç©ºé–“ç®¡ç†**: è‡ªå‹•æ¸…ç†èˆŠçš„ checkpointï¼Œç¯€çœå­˜å„²ç©ºé–“\n",
    "\n",
    "### ğŸš€ åœ¨ Google Colab ä¸­ä½¿ç”¨\n",
    "\n",
    "1. **é¦–æ¬¡é‹è¡Œ**: ç›´æ¥åŸ·è¡Œæ‰€æœ‰ cellï¼Œè¨“ç·´æœƒè‡ªå‹•é–‹å§‹\n",
    "2. **ä¸­æ–·æ¢å¾©**: å¦‚æœè¨“ç·´ä¸­æ–·ï¼Œé‡æ–°é‹è¡Œæ™‚æœƒè‡ªå‹•å¾ checkpoint æ¢å¾©\n",
    "3. **é€²åº¦æŸ¥çœ‹**: éš¨æ™‚æŸ¥çœ‹ `show_checkpoint_info()` äº†è§£ç•¶å‰ç‹€æ…‹\n",
    "4. **å¯è¦–åŒ–**: ä½¿ç”¨ `plot_training_progress()` æŸ¥çœ‹è¨“ç·´æ›²ç·š\n",
    "\n",
    "### ğŸ“ æª”æ¡ˆçµæ§‹\n",
    "\n",
    "```\n",
    "checkpoints/\n",
    "â”œâ”€â”€ latest_checkpoint.pt          # æœ€æ–°çš„ checkpoint\n",
    "â”œâ”€â”€ checkpoint_iter_X_timestamp.pt # ç‰¹å®šè¿­ä»£çš„ checkpoint\n",
    "â”œâ”€â”€ final_training_history.pkl     # å®Œæ•´è¨“ç·´æ­·å²\n",
    "â””â”€â”€ strong_reversi_model.json      # æœ€çµ‚çš„ JavaScript æ¨¡å‹\n",
    "```\n",
    "\n",
    "### ğŸ› ï¸ å¦‚ä½•ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹\n",
    "\n",
    "1. åŸ·è¡Œå®Œæ•´è¨“ç·´éç¨‹ï¼ˆå°‡ `fast_test` è¨­ç½®ç‚º `False`ï¼‰\n",
    "2. è¨“ç·´å®Œæˆå¾Œï¼Œä½ å°‡å¾—åˆ°ï¼š\n",
    "   - `strong_reversi_model.json`ï¼šåŒ…å«æ¨¡å‹æ¬Šé‡çš„ JSON æ–‡ä»¶\n",
    "   - `reversi_ai.js`ï¼šJavaScript åŒ…è£å™¨\n",
    "   - å®Œæ•´çš„ checkpoint æª”æ¡ˆç”¨æ–¼æœªä¾†ç¹¼çºŒè¨“ç·´\n",
    "\n",
    "3. åœ¨ä½ çš„ Web æ‡‰ç”¨ä¸­ä½¿ç”¨ï¼š\n",
    "   ```javascript\n",
    "   // åˆå§‹åŒ– AI\n",
    "   const ai = new ReversiAI();\n",
    "   \n",
    "   // è¼‰å…¥æ¨¡å‹æ¬Šé‡\n",
    "   await ai.loadWeights('strong_reversi_model.json');\n",
    "   \n",
    "   // ç²å–æœ€ä½³èµ°æ­¥\n",
    "   const bestMove = ai.getBestMove(boardState, currentPlayer);\n",
    "   ```\n",
    "\n",
    "### ğŸ’¡ è¨“ç·´å»ºè­°\n",
    "\n",
    "- **åˆå­¸è€…**: ä½¿ç”¨ `fast_test=True` å…ˆç†Ÿæ‚‰æµç¨‹\n",
    "- **æ­£å¼è¨“ç·´**: è¨­ç½® `fast_test=False`ï¼Œè¿­ä»£æ¬¡æ•¸å»ºè­° 15-30 æ¬¡\n",
    "- **é•·æ™‚é–“è¨“ç·´**: åœ¨ Colab Pro ä¸­é‹è¡Œï¼Œé¿å…å…è²»ç‰ˆçš„æ™‚é–“é™åˆ¶\n",
    "- **å®šæœŸæª¢æŸ¥**: ä½¿ç”¨å¯è¦–åŒ–åŠŸèƒ½ç›£æ§è¨“ç·´é€²åº¦\n",
    "\n",
    "### ğŸ”§ é€²ä¸€æ­¥æ”¹é€²\n",
    "\n",
    "- å¢åŠ è¨“ç·´è¿­ä»£æ¬¡æ•¸å’Œè‡ªæˆ‘å°å¼ˆçš„éŠæˆ²æ•¸é‡\n",
    "- èª¿æ•´ç¥ç¶“ç¶²è·¯æ¶æ§‹ï¼Œå¢åŠ ç¶²è·¯çš„æ·±åº¦å’Œå¯¬åº¦\n",
    "- å¯¦ç¾æ›´è¤‡é›œçš„æœç´¢ç®—æ³•ï¼Œå¦‚ AlphaZero ä¸­çš„ PUCT ç®—æ³•\n",
    "- æ·»åŠ æ£‹ç›¤ç‹€æ…‹å¢å¼·ï¼Œé€šéæ—‹è½‰å’Œé¡åƒä¾†å¢åŠ è¨“ç·´æ•¸æ“šçš„å¤šæ¨£æ€§\n",
    "- å¯¦ç¾å°æŠ—ä¸åŒå¼·åº¦ AI çš„è©•ä¼°ç³»çµ±\n",
    "\n",
    "### âš ï¸ æ³¨æ„äº‹é …\n",
    "\n",
    "- è¨“ç·´éç¨‹è€—æ™‚è¼ƒé•·ï¼Œå»ºè­°åœ¨æœ‰ç©©å®šç¶²è·¯çš„ç’°å¢ƒä¸­é€²è¡Œ\n",
    "- åœ¨ Colab ä¸­è¨“ç·´æ™‚ï¼Œcheckpoint æœƒè‡ªå‹•ä¿å­˜åˆ° Google Drive\n",
    "- å®šæœŸä¸‹è¼‰é‡è¦çš„ checkpoint åˆ°æœ¬åœ°ä½œç‚ºå‚™ä»½\n",
    "- å¦‚æœè¨˜æ†¶é«”ä¸è¶³ï¼Œå¯ä»¥æ¸›å°‘ `games_per_iteration` æˆ– `batch_size`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
