{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"管理訓練過程中的checkpoint保存和載入\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, model, optimizer, iteration, total_iterations, \n",
    "                       training_history, metadata=None):\n",
    "        \"\"\"保存訓練checkpoint\"\"\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_name = f\"checkpoint_iter_{iteration:03d}_{timestamp}.pt\"\n",
    "        checkpoint_path = self.checkpoint_dir / checkpoint_name\n",
    "        \n",
    "        checkpoint_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'iteration': iteration,\n",
    "            'total_iterations': total_iterations,\n",
    "            'training_history': training_history,\n",
    "            'timestamp': timestamp,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint_data, checkpoint_path)\n",
    "        \n",
    "        # 同時保存最新的checkpoint為固定名稱\n",
    "        latest_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "        torch.save(checkpoint_data, latest_path)\n",
    "        \n",
    "        # 保存訓練歷史為單獨文件\n",
    "        history_path = self.checkpoint_dir / f\"history_iter_{iteration:03d}.pkl\"\n",
    "        with open(history_path, 'wb') as f:\n",
    "            pickle.dump(training_history, f)\n",
    "        \n",
    "        print(f\"💾 Checkpoint 已保存: {checkpoint_name}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_latest_checkpoint(self, model, optimizer):\n",
    "        \"\"\"載入最新的checkpoint\"\"\"\n",
    "        latest_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "        \n",
    "        if not latest_path.exists():\n",
    "            print(\"📭 未找到checkpoint，將從頭開始訓練\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(latest_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "            print(f\"📥 已載入checkpoint: 迭代 {checkpoint['iteration']}/{checkpoint['total_iterations']}\")\n",
    "            print(f\"📅 保存時間: {checkpoint['timestamp']}\")\n",
    "            \n",
    "            return checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 載入checkpoint失敗: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"列出所有可用的checkpoint\"\"\"\n",
    "        checkpoint_files = list(self.checkpoint_dir.glob(\"checkpoint_iter_*.pt\"))\n",
    "        checkpoint_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        return checkpoint_files\n",
    "    \n",
    "    def clean_old_checkpoints(self, keep_count=5):\n",
    "        \"\"\"清理舊的checkpoint，只保留最新的幾個\"\"\"\n",
    "        checkpoint_files = self.list_checkpoints()\n",
    "        \n",
    "        if len(checkpoint_files) > keep_count:\n",
    "            files_to_delete = checkpoint_files[keep_count:]\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    # 同時刪除對應的歷史文件\n",
    "                    history_file = file_path.parent / file_path.name.replace(\"checkpoint_\", \"history_\")\n",
    "                    if history_file.exists():\n",
    "                        os.remove(history_file)\n",
    "                    print(f\"🗑️  已清理舊checkpoint: {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  清理checkpoint失敗 {file_path.name}: {e}\")\n",
    "    \n",
    "    def get_checkpoint_info(self):\n",
    "        \"\"\"獲取checkpoint信息\"\"\"\n",
    "        latest_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "        \n",
    "        if not latest_path.exists():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(latest_path, map_location='cpu')\n",
    "            return {\n",
    "                'iteration': checkpoint['iteration'],\n",
    "                'total_iterations': checkpoint['total_iterations'],\n",
    "                'timestamp': checkpoint['timestamp'],\n",
    "                'metadata': checkpoint.get('metadata', {}),\n",
    "                'training_history': checkpoint.get('training_history', {})\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 讀取checkpoint信息失敗: {e}\")\n",
    "            return None\n",
    "\n",
    "# 初始化checkpoint管理器\n",
    "checkpoint_manager = CheckpointManager(CHECKPOINT_DIR)\n",
    "print(f\"📂 Checkpoint管理器已初始化: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f104552",
   "metadata": {},
   "source": [
    "# 黑白棋 (Reversi) 高勝率 AI 訓練\n",
    "\n",
    "本筆記本用於訓練一個非常不容易輸的黑白棋 AI 模型，訓練完成後將模型轉換為 JavaScript 格式以便在 Web 應用程式中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e968e2e",
   "metadata": {},
   "source": [
    "## 環境設置\n",
    "\n",
    "首先，我們需要確認 GPU 可用，並安裝必要的函式庫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 是否可用: False\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# 檢查 GPU 是否可用\n",
    "import torch\n",
    "print(\"GPU 是否可用:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 型號:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# 安裝必要的函式庫\n",
    "!pip install numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查 GPU 是否可用\n",
    "import torch\n",
    "print(\"GPU 是否可用:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 型號:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# 安裝必要的函式庫\n",
    "!pip install numpy matplotlib tqdm\n",
    "\n",
    "# 檢查是否在 Colab 環境中運行\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"在 Google Colab 環境中運行\")\n",
    "    # 在 Colab 中掛載 Google Drive 以保存 checkpoint\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/reversi_checkpoints/'\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"在本地環境中運行\")\n",
    "    CHECKPOINT_DIR = './checkpoints/'\n",
    "\n",
    "# 創建 checkpoint 目錄\n",
    "import os\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoint 將保存到: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206e740",
   "metadata": {},
   "source": [
    "## 黑白棋遊戲實現\n",
    "\n",
    "接下來我們實現黑白棋的遊戲規則，包括棋盤初始化、走步驗證、棋子翻轉等功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7db255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class ReversiGame:\n",
    "    def __init__(self):\n",
    "        # 初始化 8x8 棋盤，0=空，1=黑，2=白\n",
    "        self.board = np.zeros((8, 8), dtype=np.int32)\n",
    "        \n",
    "        # 設置初始四個棋子\n",
    "        self.board[3][3] = 2  # 白\n",
    "        self.board[3][4] = 1  # 黑\n",
    "        self.board[4][3] = 1  # 黑\n",
    "        self.board[4][4] = 2  # 白\n",
    "        \n",
    "        # 當前玩家 (1=黑, 2=白)\n",
    "        self.current_player = 1\n",
    "        \n",
    "        # 遊戲是否結束\n",
    "        self.game_over = False\n",
    "        \n",
    "        # 方向向量\n",
    "        self.directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), \n",
    "                         (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "    \n",
    "    def clone(self):\n",
    "        \"\"\"複製當前遊戲狀態\"\"\"\n",
    "        new_game = ReversiGame()\n",
    "        new_game.board = self.board.copy()\n",
    "        new_game.current_player = self.current_player\n",
    "        new_game.game_over = self.game_over\n",
    "        return new_game\n",
    "    \n",
    "    def is_valid_move(self, row, col):\n",
    "        \"\"\"檢查當前玩家在指定位置是否可以下棋\"\"\"\n",
    "        # 如果格子已被佔用，則不合法\n",
    "        if self.board[row][col] != 0:\n",
    "            return False\n",
    "        \n",
    "        # 對手的棋子顏色\n",
    "        opponent = 3 - self.current_player\n",
    "        \n",
    "        # 檢查八個方向\n",
    "        for dr, dc in self.directions:\n",
    "            r, c = row + dr, col + dc\n",
    "            # 確認有對手的棋子相鄰\n",
    "            if 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == opponent:\n",
    "                # 沿著這個方向繼續前進\n",
    "                r, c = r + dr, c + dc\n",
    "                while 0 <= r < 8 and 0 <= c < 8:\n",
    "                    # 如果遇到空格，這個方向無法翻轉棋子\n",
    "                    if self.board[r][c] == 0:\n",
    "                        break\n",
    "                    # 如果遇到自己的棋子，則可以翻轉，走步合法\n",
    "                    if self.board[r][c] == self.current_player:\n",
    "                        return True\n",
    "                    # 繼續沿著這個方向\n",
    "                    r, c = r + dr, c + dc\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_valid_moves(self):\n",
    "        \"\"\"獲取當前玩家的所有合法走步\"\"\"\n",
    "        valid_moves = []\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                if self.is_valid_move(row, col):\n",
    "                    valid_moves.append((row, col))\n",
    "        return valid_moves\n",
    "    \n",
    "    def make_move(self, row, col):\n",
    "        \"\"\"執行一步走步\"\"\"\n",
    "        if not self.is_valid_move(row, col):\n",
    "            return False\n",
    "        \n",
    "        # 放置棋子\n",
    "        self.board[row][col] = self.current_player\n",
    "        \n",
    "        # 翻轉對手的棋子\n",
    "        opponent = 3 - self.current_player\n",
    "        for dr, dc in self.directions:\n",
    "            pieces_to_flip = []\n",
    "            r, c = row + dr, col + dc\n",
    "            \n",
    "            # 檢查該方向是否有可以翻轉的棋子\n",
    "            while 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == opponent:\n",
    "                pieces_to_flip.append((r, c))\n",
    "                r, c = r + dr, c + dc\n",
    "                \n",
    "                # 如果遇到自己的棋子，翻轉中間所有對手的棋子\n",
    "                if 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == self.current_player:\n",
    "                    for flip_r, flip_c in pieces_to_flip:\n",
    "                        self.board[flip_r][flip_c] = self.current_player\n",
    "                    break\n",
    "        \n",
    "        # 切換玩家\n",
    "        self.current_player = opponent\n",
    "        \n",
    "        # 檢查下一個玩家是否有合法走步\n",
    "        if not self.get_valid_moves():\n",
    "            # 如果下一個玩家沒有合法走步，檢查當前玩家是否有合法走步\n",
    "            self.current_player = 3 - self.current_player\n",
    "            if not self.get_valid_moves():\n",
    "                # 如果雙方都沒有合法走步，遊戲結束\n",
    "                self.game_over = True\n",
    "            # 否則跳過當前玩家\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_score(self):\n",
    "        \"\"\"獲取當前分數\"\"\"\n",
    "        black_count = np.sum(self.board == 1)\n",
    "        white_count = np.sum(self.board == 2)\n",
    "        return black_count, white_count\n",
    "    \n",
    "    def get_winner(self):\n",
    "        \"\"\"獲取贏家 (1=黑, 2=白, 0=平局)\"\"\"\n",
    "        black_count, white_count = self.get_score()\n",
    "        if black_count > white_count:\n",
    "            return 1\n",
    "        elif white_count > black_count:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def print_board(self):\n",
    "        \"\"\"打印棋盤\"\"\"\n",
    "        symbols = {0: '.', 1: '●', 2: '○'}\n",
    "        print('  0 1 2 3 4 5 6 7')\n",
    "        for i in range(8):\n",
    "            row_str = f\"{i} \"\n",
    "            for j in range(8):\n",
    "                row_str += symbols[self.board[i][j]] + ' '\n",
    "            print(row_str)\n",
    "        black, white = self.get_score()\n",
    "        print(f\"黑棋: {black}, 白棋: {white}\")\n",
    "        print(f\"當前玩家: {'黑' if self.current_player == 1 else '白'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試遊戲實現\n",
    "game = ReversiGame()\n",
    "game.print_board()\n",
    "\n",
    "# 顯示合法走步\n",
    "valid_moves = game.get_valid_moves()\n",
    "print(f\"合法走步: {valid_moves}\")\n",
    "\n",
    "# 執行一步走步\n",
    "if valid_moves:\n",
    "    game.make_move(valid_moves[0][0], valid_moves[0][1])\n",
    "    print(\"\\n執行走步後:\")\n",
    "    game.print_board()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef6190",
   "metadata": {},
   "source": [
    "## 神經網路模型\n",
    "\n",
    "我們將建立一個深度學習模型，該模型接收棋盤狀態作為輸入，並輸出每個位置的走步價值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5121ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReversiNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReversiNet, self).__init__()\n",
    "        # 棋盤狀態輸入: 3 通道 (自己的棋子、對手的棋子、空位) x 8 x 8\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 殘差連接\n",
    "        self.residual1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        self.residual2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        \n",
    "        # 策略頭 (走步概率)\n",
    "        self.policy_conv = nn.Conv2d(128, 32, kernel_size=1)\n",
    "        self.policy_fc = nn.Linear(32 * 8 * 8, 64)  # 8x8 棋盤的所有位置\n",
    "        \n",
    "        # 價值頭 (勝率評估)\n",
    "        self.value_conv = nn.Conv2d(128, 32, kernel_size=1)\n",
    "        self.value_fc1 = nn.Linear(32 * 8 * 8, 64)\n",
    "        self.value_fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "        # Batch Normalization 層\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn_policy = nn.BatchNorm2d(32)\n",
    "        self.bn_value = nn.BatchNorm2d(32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 卷積層\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # 殘差連接\n",
    "        x_res = x\n",
    "        x = F.relu(x + self.residual1(x_res))\n",
    "        x_res = x\n",
    "        x = F.relu(x + self.residual2(x_res))\n",
    "        \n",
    "        # 策略頭\n",
    "        policy = F.relu(self.bn_policy(self.policy_conv(x)))\n",
    "        policy = policy.view(-1, 32 * 8 * 8)\n",
    "        policy = self.policy_fc(policy)\n",
    "        \n",
    "        # 價值頭\n",
    "        value = F.relu(self.bn_value(self.value_conv(x)))\n",
    "        value = value.view(-1, 32 * 8 * 8)\n",
    "        value = F.relu(self.value_fc1(value))\n",
    "        value = torch.tanh(self.value_fc2(value))  # 輸出範圍 [-1, 1]\n",
    "        \n",
    "        return policy, value\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ReversiNet().to(device)\n",
    "print(f\"模型已建立，使用裝置: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3cafb",
   "metadata": {},
   "source": [
    "## 自我對弈與強化學習\n",
    "\n",
    "接下來我們使用強化學習中的蒙地卡羅樹搜索 (MCTS) 來通過自我對弈生成訓練數據，然後使用這些數據訓練神經網路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51350ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, game, parent=None, move=None):\n",
    "        self.game = game\n",
    "        self.parent = parent\n",
    "        self.move = move  # 到達此節點的走步\n",
    "        self.children = {}  # 子節點\n",
    "        self.visits = 0  # 訪問次數\n",
    "        self.value = 0.0  # 總獎勵\n",
    "        self.untried_moves = game.get_valid_moves()  # 未嘗試的走步\n",
    "    \n",
    "    def select_child(self, c_param=1.4):\n",
    "        \"\"\"選擇最具潛力的子節點 (UCB1)\"\"\"\n",
    "        # 選擇 UCB 值最高的子節點\n",
    "        return max(self.children.values(),\n",
    "                  key=lambda node: node.value / (node.visits + 1e-8) + \\\n",
    "                  c_param * math.sqrt(2 * math.log(self.visits + 1) / (node.visits + 1e-8)))\n",
    "    \n",
    "    def expand(self):\n",
    "        \"\"\"擴展一個未嘗試的走步\"\"\"\n",
    "        row, col = self.untried_moves.pop()\n",
    "        next_game = self.game.clone()\n",
    "        next_game.make_move(row, col)\n",
    "        child_node = MCTSNode(next_game, parent=self, move=(row, col))\n",
    "        self.children[(row, col)] = child_node\n",
    "        return child_node\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        \"\"\"檢查是否所有可能的走步都已嘗試\"\"\"\n",
    "        return len(self.untried_moves) == 0\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        \"\"\"檢查是否為終止節點\"\"\"\n",
    "        return self.game.game_over\n",
    "    \n",
    "    def rollout(self, model=None):\n",
    "        \"\"\"隨機模擬遊戲直至結束\"\"\"\n",
    "        # 複製當前遊戲進行模擬\n",
    "        sim_game = self.game.clone()\n",
    "        \n",
    "        # 使用神經網路指導的走步選擇\n",
    "        if model is not None and not sim_game.game_over:\n",
    "            while not sim_game.game_over:\n",
    "                valid_moves = sim_game.get_valid_moves()\n",
    "                if not valid_moves:\n",
    "                    # 切換玩家\n",
    "                    sim_game.current_player = 3 - sim_game.current_player\n",
    "                    valid_moves = sim_game.get_valid_moves()\n",
    "                    if not valid_moves:\n",
    "                        sim_game.game_over = True\n",
    "                        break\n",
    "                    continue\n",
    "                \n",
    "                # 將棋盤轉換為神經網路輸入格式\n",
    "                board_tensor = self.prepare_input(sim_game)\n",
    "                \n",
    "                # 獲取模型預測\n",
    "                policy, _ = model(board_tensor)\n",
    "                \n",
    "                # 對合法走步的概率進行過濾\n",
    "                move_probs = torch.zeros(64)\n",
    "                for row, col in valid_moves:\n",
    "                    move_probs[row * 8 + col] = policy[0, row * 8 + col].item()\n",
    "                \n",
    "                # 做Softmax獲取概率分佈\n",
    "                move_probs = F.softmax(move_probs, dim=0)\n",
    "                \n",
    "                # 根據概率選擇走步\n",
    "                move_idx = torch.multinomial(move_probs, 1).item()\n",
    "                row, col = move_idx // 8, move_idx % 8\n",
    "                \n",
    "                # 確保選擇的走步是合法的\n",
    "                if (row, col) in valid_moves:\n",
    "                    sim_game.make_move(row, col)\n",
    "                else:\n",
    "                    # 如果選擇的走步不合法，則隨機選擇一個合法走步\n",
    "                    row, col = random.choice(valid_moves)\n",
    "                    sim_game.make_move(row, col)\n",
    "        else:\n",
    "            # 隨機走步模擬\n",
    "            while not sim_game.game_over:\n",
    "                valid_moves = sim_game.get_valid_moves()\n",
    "                if not valid_moves:\n",
    "                    # 切換玩家\n",
    "                    sim_game.current_player = 3 - sim_game.current_player\n",
    "                    valid_moves = sim_game.get_valid_moves()\n",
    "                    if not valid_moves:\n",
    "                        sim_game.game_over = True\n",
    "                        break\n",
    "                    continue\n",
    "                \n",
    "                # 隨機選擇一個合法走步\n",
    "                row, col = random.choice(valid_moves)\n",
    "                sim_game.make_move(row, col)\n",
    "        \n",
    "        # 獲取遊戲結果\n",
    "        winner = sim_game.get_winner()\n",
    "        \n",
    "        # 從當前玩家的角度返回獎勵\n",
    "        current_player = self.game.current_player\n",
    "        if winner == 0:  # 平局\n",
    "            return 0.5\n",
    "        elif winner == current_player:  # 當前玩家贏\n",
    "            return 1.0\n",
    "        else:  # 當前玩家輸\n",
    "            return 0.0\n",
    "    \n",
    "    def backpropagate(self, result):\n",
    "        \"\"\"向上更新統計信息\"\"\"\n",
    "        self.visits += 1\n",
    "        self.value += result\n",
    "        \n",
    "        # 向上傳遞結果\n",
    "        if self.parent:\n",
    "            # 從對手的角度計算獎勵（翻轉結果）\n",
    "            self.parent.backpropagate(1 - result)\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_input(game):\n",
    "        \"\"\"將遊戲狀態轉換為神經網路輸入格式\"\"\"\n",
    "        board = game.board\n",
    "        current_player = game.current_player\n",
    "        opponent = 3 - current_player\n",
    "        \n",
    "        # 3 通道: 自己的棋子、對手的棋子、空位\n",
    "        player_channel = (board == current_player).astype(np.float32)\n",
    "        opponent_channel = (board == opponent).astype(np.float32)\n",
    "        empty_channel = (board == 0).astype(np.float32)\n",
    "        \n",
    "        # 組合成 3x8x8 的張量\n",
    "        state = np.stack([player_channel, opponent_channel, empty_channel])\n",
    "        \n",
    "        # 轉換為 PyTorch 張量並加入批次維度\n",
    "        tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        return tensor.to(device)\n",
    "\n",
    "def mcts_search(game, model, num_simulations=800):\n",
    "    \"\"\"執行蒙地卡羅樹搜索\"\"\"\n",
    "    root = MCTSNode(game)\n",
    "    \n",
    "    for _ in range(num_simulations):\n",
    "        # 選擇\n",
    "        node = root\n",
    "        while node.is_fully_expanded() and not node.is_terminal():\n",
    "            node = node.select_child()\n",
    "        \n",
    "        # 擴展\n",
    "        if not node.is_terminal() and not node.is_fully_expanded():\n",
    "            node = node.expand()\n",
    "        \n",
    "        # 模擬\n",
    "        result = node.rollout(model)\n",
    "        \n",
    "        # 回溯\n",
    "        node.backpropagate(result)\n",
    "    \n",
    "    # 選擇訪問次數最多的子節點\n",
    "    if root.children:\n",
    "        best_move = max(root.children.items(),\n",
    "                        key=lambda item: item[1].visits)[0]\n",
    "        return best_move\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def self_play_game(model, temperature=1.0):\n",
    "    \"\"\"進行一場自我對弈，生成訓練數據\"\"\"\n",
    "    game = ReversiGame()\n",
    "    states = []  # 記錄遊戲狀態\n",
    "    policies = []  # 記錄走步概率分佈\n",
    "    player_turns = []  # 記錄當前玩家\n",
    "    \n",
    "    while not game.game_over:\n",
    "        # 獲取當前玩家\n",
    "        current_player = game.current_player\n",
    "        player_turns.append(current_player)\n",
    "        \n",
    "        # 獲取合法走步\n",
    "        valid_moves = game.get_valid_moves()\n",
    "        \n",
    "        # 如果沒有合法走步，切換玩家\n",
    "        if not valid_moves:\n",
    "            game.current_player = 3 - game.current_player\n",
    "            continue\n",
    "        \n",
    "        # 保存當前狀態\n",
    "        state = MCTSNode.prepare_input(game).cpu().squeeze(0).numpy()\n",
    "        states.append(state)\n",
    "        \n",
    "        # 將遊戲狀態送入模型，獲取策略分佈和價值評估\n",
    "        policy, _ = model(MCTSNode.prepare_input(game))\n",
    "        policy = policy.detach().cpu().squeeze(0).numpy()\n",
    "        \n",
    "        # 執行MCTS搜索\n",
    "        move = mcts_search(game, model, num_simulations=400)\n",
    "        \n",
    "        # 創建走步概率分佈\n",
    "        mcts_policy = np.zeros(64, dtype=np.float32)\n",
    "        if move is not None:\n",
    "            row, col = move\n",
    "            mcts_policy[row * 8 + col] = 1.0\n",
    "        \n",
    "        policies.append(mcts_policy)\n",
    "        \n",
    "        # 執行選擇的走步\n",
    "        if move is not None:\n",
    "            game.make_move(move[0], move[1])\n",
    "    \n",
    "    # 獲取比賽結果\n",
    "    winner = game.get_winner()\n",
    "    \n",
    "    # 計算每個狀態的獎勵\n",
    "    rewards = []\n",
    "    for player in player_turns:\n",
    "        if winner == 0:  # 平局\n",
    "            rewards.append(0.0)\n",
    "        elif winner == player:  # 獲勝\n",
    "            rewards.append(1.0)\n",
    "        else:  # 失敗\n",
    "            rewards.append(-1.0)\n",
    "    \n",
    "    # 將數據轉換為張量格式\n",
    "    states = np.array(states)\n",
    "    policies = np.array(policies)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    \n",
    "    return states, policies, rewards\n",
    "\n",
    "def generate_training_data(model, num_games=100):\n",
    "    \"\"\"生成訓練數據\"\"\"\n",
    "    all_states = []\n",
    "    all_policies = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    for _ in tqdm(range(num_games)):\n",
    "        states, policies, rewards = self_play_game(model)\n",
    "        all_states.append(states)\n",
    "        all_policies.append(policies)\n",
    "        all_rewards.append(rewards)\n",
    "    \n",
    "    # 合併數據\n",
    "    all_states = np.concatenate(all_states)\n",
    "    all_policies = np.concatenate(all_policies)\n",
    "    all_rewards = np.concatenate(all_rewards)\n",
    "    \n",
    "    return all_states, all_policies, all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90919b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_model(model, states, policies, rewards, batch_size=64, epochs=5):\n",
    "    \"\"\"訓練模型\"\"\"\n",
    "    # 創建數據集\n",
    "    X = torch.FloatTensor(states).to(device)\n",
    "    policy_y = torch.FloatTensor(policies).to(device)\n",
    "    value_y = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X, policy_y, value_y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 定義優化器和損失函數\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    policy_criterion = nn.MSELoss()\n",
    "    value_criterion = nn.MSELoss()\n",
    "    \n",
    "    # 訓練循環\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        policy_loss_sum = 0\n",
    "        value_loss_sum = 0\n",
    "        \n",
    "        for batch_x, batch_policy_y, batch_value_y in dataloader:\n",
    "            # 前向傳播\n",
    "            policy_pred, value_pred = model(batch_x)\n",
    "            \n",
    "            # 計算損失\n",
    "            policy_loss = policy_criterion(policy_pred, batch_policy_y)\n",
    "            value_loss = value_criterion(value_pred, batch_value_y)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # 反向傳播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 累計損失\n",
    "            total_loss += loss.item()\n",
    "            policy_loss_sum += policy_loss.item()\n",
    "            value_loss_sum += value_loss.item()\n",
    "        \n",
    "        # 打印訓練進度\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_policy_loss = policy_loss_sum / len(dataloader)\n",
    "        avg_value_loss = value_loss_sum / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Policy Loss: {avg_policy_loss:.4f}, Value Loss: {avg_value_loss:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58903a3d",
   "metadata": {},
   "source": [
    "## 模型訓練與評估\n",
    "\n",
    "現在我們來實現模型的訓練和評估過程，通過多次迭代來提高模型性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a4b9a",
   "metadata": {},
   "source": [
    "## Checkpoint 功能\n",
    "\n",
    "為了在 Colab 中安全訓練並避免因中斷而丟失進度，我們實現了完整的 checkpoint 功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a850d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class TrainingCheckpoint:\n",
    "    \"\"\"訓練檢查點管理器\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir=CHECKPOINT_DIR):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, model, optimizer, iteration, total_iterations, \n",
    "                       training_history, metadata=None):\n",
    "        \"\"\"保存訓練檢查點\"\"\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_path = self.checkpoint_dir / f\"checkpoint_iter_{iteration}_{timestamp}.pt\"\n",
    "        \n",
    "        checkpoint = {\n",
    "            'iteration': iteration,\n",
    "            'total_iterations': total_iterations,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'training_history': training_history,\n",
    "            'timestamp': timestamp,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # 也保存一個 \"latest\" 版本\n",
    "        latest_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        print(f\"✅ Checkpoint 已保存: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path=None, model=None, optimizer=None):\n",
    "        \"\"\"載入訓練檢查點\"\"\"\n",
    "        if checkpoint_path is None:\n",
    "            # 載入最新的 checkpoint\n",
    "            checkpoint_path = self.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "            \n",
    "        if not checkpoint_path.exists():\n",
    "            print(\"❌ 沒有找到 checkpoint 檔案\")\n",
    "            return None\n",
    "            \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        if model is not None:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "        print(f\"✅ Checkpoint 已載入: {checkpoint_path}\")\n",
    "        print(f\"   迭代: {checkpoint['iteration']}/{checkpoint['total_iterations']}\")\n",
    "        print(f\"   時間戳: {checkpoint['timestamp']}\")\n",
    "        \n",
    "        return checkpoint\n",
    "    \n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"列出所有可用的 checkpoint\"\"\"\n",
    "        checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pt\"))\n",
    "        checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        \n",
    "        print(f\"找到 {len(checkpoints)} 個 checkpoint:\")\n",
    "        for i, cp in enumerate(checkpoints[:10]):  # 只顯示最新的 10 個\n",
    "            stat = cp.stat()\n",
    "            size_mb = stat.st_size / (1024 * 1024)\n",
    "            mtime = datetime.datetime.fromtimestamp(stat.st_mtime)\n",
    "            print(f\"  {i+1}. {cp.name} ({size_mb:.1f}MB, {mtime.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "            \n",
    "        return checkpoints\n",
    "    \n",
    "    def clean_old_checkpoints(self, keep_count=5):\n",
    "        \"\"\"清理舊的 checkpoint，只保留最新的幾個\"\"\"\n",
    "        checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pt\"))\n",
    "        checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "        \n",
    "        # 保留最新的 keep_count 個，刪除其餘的\n",
    "        to_delete = checkpoints[keep_count:]\n",
    "        for cp in to_delete:\n",
    "            cp.unlink()\n",
    "            print(f\"🗑️  已刪除舊 checkpoint: {cp.name}\")\n",
    "        \n",
    "        print(f\"✅ 清理完成，保留了 {min(len(checkpoints), keep_count)} 個最新的 checkpoint\")\n",
    "\n",
    "# 初始化 checkpoint 管理器\n",
    "checkpoint_manager = TrainingCheckpoint()\n",
    "print(\"Checkpoint 管理器已初始化\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_strong_ai(iterations=10, games_per_iteration=100, training_epochs=5):\n",
    "    \"\"\"訓練一個強大的 AI\"\"\"\n",
    "    model = ReversiNet().to(device)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        print(f\"\\n開始訓練迭代 {iteration+1}/{iterations}\")\n",
    "        \n",
    "        # 生成自我對弈數據\n",
    "        print(\"正在生成訓練數據...\")\n",
    "        states, policies, rewards = generate_training_data(model, num_games=games_per_iteration)\n",
    "        \n",
    "        # 訓練模型\n",
    "        print(\"\\n訓練模型...\")\n",
    "        model = train_model(model, states, policies, rewards, epochs=training_epochs)\n",
    "        \n",
    "        # 保存模型\n",
    "        torch.save(model.state_dict(), f'strong_reversi_model_iter_{iteration+1}.pth')\n",
    "        print(f\"模型已保存為 strong_reversi_model_iter_{iteration+1}.pth\")\n",
    "        \n",
    "        # 評估模型\n",
    "        # （此處可以添加對抗隨機AI或評估模型的代碼）\n",
    "        \n",
    "    return model\n",
    "\n",
    "# 執行完整訓練過程 (慎重執行，耗時較長)\n",
    "# strong_model = train_strong_ai(iterations=5, games_per_iteration=20, training_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_against_random(model, num_games=100):\n",
    "    \"\"\"評估模型對抗隨機AI的表現\"\"\"\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for game_idx in tqdm(range(num_games)):\n",
    "        game = ReversiGame()\n",
    "        model_plays_black = game_idx % 2 == 0  # 輪流執黑執白\n",
    "        \n",
    "        while not game.game_over:\n",
    "            current_player = game.current_player\n",
    "            valid_moves = game.get_valid_moves()\n",
    "            \n",
    "            if not valid_moves:\n",
    "                # 如果當前玩家沒有合法走步，切換玩家\n",
    "                game.current_player = 3 - game.current_player\n",
    "                valid_moves = game.get_valid_moves()\n",
    "                \n",
    "                # 如果兩位玩家都沒有合法走步，遊戲結束\n",
    "                if not valid_moves:\n",
    "                    game.game_over = True\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            if (model_plays_black and current_player == 1) or (not model_plays_black and current_player == 2):\n",
    "                # 模型的回合\n",
    "                move = mcts_search(game, model, num_simulations=100)\n",
    "                if move:\n",
    "                    game.make_move(move[0], move[1])\n",
    "            else:\n",
    "                # 隨機AI的回合\n",
    "                move = random.choice(valid_moves)\n",
    "                game.make_move(move[0], move[1])\n",
    "        \n",
    "        # 計算勝負\n",
    "        winner = game.get_winner()\n",
    "        if winner == 0:  # 平局\n",
    "            draws += 1\n",
    "        elif (winner == 1 and model_plays_black) or (winner == 2 and not model_plays_black):\n",
    "            wins += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "    \n",
    "    win_rate = wins / num_games\n",
    "    loss_rate = losses / num_games\n",
    "    draw_rate = draws / num_games\n",
    "    \n",
    "    print(f\"評估結果 (共 {num_games} 場)\")\n",
    "    print(f\"勝率: {win_rate:.2%} ({wins} 勝)\")\n",
    "    print(f\"敗率: {loss_rate:.2%} ({losses} 敗)\")\n",
    "    print(f\"平局: {draw_rate:.2%} ({draws} 平)\")\n",
    "    \n",
    "    return win_rate, loss_rate, draw_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75929254",
   "metadata": {},
   "source": [
    "## 將模型轉換為 JavaScript 格式\n",
    "\n",
    "為了在瀏覽器中使用我們訓練的模型，我們需要將其轉換為 JavaScript 格式。這裡我們將提取模型權重並將其導出為 JSON 格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "def convert_model_to_js(model, filename=\"strong_reversi_model.json\"):\n",
    "    \"\"\"將模型權重轉換為 JavaScript 兼容的 JSON 格式\"\"\"\n",
    "    model_weights = {}\n",
    "    \n",
    "    # 遍歷模型參數\n",
    "    for name, param in model.named_parameters():\n",
    "        # 將張量轉換為可序列化的列表\n",
    "        model_weights[name] = param.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    # 保存為 JSON 文件\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(model_weights, f)\n",
    "    \n",
    "    print(f\"模型已轉換並保存為 {filename}\")\n",
    "    \n",
    "    # 準備一個 JavaScript 包裝器，以便於在瀏覽器中使用\n",
    "    js_wrapper = f\"\"\"\n",
    "    // 黑白棋 AI 模型包裝器\n",
    "    class ReversiAI {{\n",
    "        constructor() {{\n",
    "            // 載入模型權重 (這將由另一個腳本填充)\n",
    "            this.weights = null;\n",
    "        }}\n",
    "        \n",
    "        // 載入模型權重\n",
    "        async loadWeights(url) {{\n",
    "            const response = await fetch(url);\n",
    "            this.weights = await response.json();\n",
    "            console.log('模型權重已載入');\n",
    "            return true;\n",
    "        }}\n",
    "        \n",
    "        // 獲取走步 (在這裡實現模型推理)\n",
    "        getBestMove(board, currentPlayer) {{\n",
    "            // 在此處實現模型推理邏輯\n",
    "            // ...\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    // 導出供全局使用\n",
    "    if (typeof module !== 'undefined') {{\n",
    "        module.exports = {{ ReversiAI }};\n",
    "    }} else {{\n",
    "        window.ReversiAI = ReversiAI;\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 保存 JavaScript 包裝器\n",
    "    with open(\"reversi_ai.js\", 'w') as f:\n",
    "        f.write(js_wrapper)\n",
    "    \n",
    "    print(\"JavaScript 包裝器已保存為 reversi_ai.js\")\n",
    "    \n",
    "    return model_weights\n",
    "\n",
    "def train_strong_ai_with_checkpoint(iterations=10, games_per_iteration=100, training_epochs=5, \n",
    "                                   resume_from_checkpoint=True, save_every=1):\n",
    "    \"\"\"訓練一個強大的 AI，支援 checkpoint 功能\"\"\"\n",
    "    \n",
    "    # 初始化模型和優化器\n",
    "    model = ReversiNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # 訓練歷史記錄\n",
    "    training_history = {\n",
    "        'losses': [],\n",
    "        'policy_losses': [],\n",
    "        'value_losses': [],\n",
    "        'win_rates': [],\n",
    "        'iterations_completed': []\n",
    "    }\n",
    "    \n",
    "    start_iteration = 0\n",
    "    \n",
    "    # 嘗試從 checkpoint 恢復\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = checkpoint_manager.load_checkpoint(model=model, optimizer=optimizer)\n",
    "        if checkpoint is not None:\n",
    "            start_iteration = checkpoint['iteration']\n",
    "            training_history = checkpoint['training_history']\n",
    "            print(f\"🔄 從迭代 {start_iteration} 恢復訓練\")\n",
    "        else:\n",
    "            print(\"🆕 開始新的訓練\")\n",
    "    \n",
    "    try:\n",
    "        for iteration in range(start_iteration, iterations):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"開始訓練迭代 {iteration+1}/{iterations}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # 生成自我對弈數據\n",
    "            print(\"🎮 正在生成訓練數據...\")\n",
    "            states, policies, rewards = generate_training_data(model, num_games=games_per_iteration)\n",
    "            print(f\"📊 生成了 {len(states)} 個訓練樣本\")\n",
    "            \n",
    "            # 訓練模型\n",
    "            print(\"\\n🧠 訓練模型...\")\n",
    "            model, epoch_losses = train_model_with_history(model, optimizer, states, policies, rewards, epochs=training_epochs)\n",
    "            \n",
    "            # 記錄訓練歷史\n",
    "            training_history['losses'].extend(epoch_losses['total'])\n",
    "            training_history['policy_losses'].extend(epoch_losses['policy'])\n",
    "            training_history['value_losses'].extend(epoch_losses['value'])\n",
    "            training_history['iterations_completed'].append(iteration + 1)\n",
    "            \n",
    "            # 評估模型（每 2 個迭代評估一次）\n",
    "            if (iteration + 1) % 2 == 0:\n",
    "                print(\"\\n📈 評估模型性能...\")\n",
    "                win_rate, _, _ = evaluate_against_random(model, num_games=20)\n",
    "                training_history['win_rates'].append(win_rate)\n",
    "                print(f\"當前勝率: {win_rate:.2%}\")\n",
    "            \n",
    "            # 保存 checkpoint\n",
    "            if (iteration + 1) % save_every == 0:\n",
    "                metadata = {\n",
    "                    'games_per_iteration': games_per_iteration,\n",
    "                    'training_epochs': training_epochs,\n",
    "                    'current_iteration': iteration + 1,\n",
    "                    'total_iterations': iterations\n",
    "                }\n",
    "                \n",
    "                checkpoint_manager.save_checkpoint(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    iteration=iteration + 1,\n",
    "                    total_iterations=iterations,\n",
    "                    training_history=training_history,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                \n",
    "                # 清理舊的 checkpoint（保留最新的 5 個）\n",
    "                if (iteration + 1) % 3 == 0:\n",
    "                    checkpoint_manager.clean_old_checkpoints(keep_count=5)\n",
    "            \n",
    "            # 顯示當前進度\n",
    "            print(f\"\\n✅ 迭代 {iteration+1}/{iterations} 完成\")\n",
    "            if training_history['losses']:\n",
    "                recent_loss = training_history['losses'][-1]\n",
    "                print(f\"最近損失: {recent_loss:.4f}\")\n",
    "            \n",
    "            # 在 Colab 中顯示進度\n",
    "            if IN_COLAB:\n",
    "                progress = (iteration + 1) / iterations * 100\n",
    "                print(f\"總進度: {progress:.1f}%\")\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️  訓練被用戶中斷\")\n",
    "        # 保存中斷時的 checkpoint\n",
    "        metadata = {\n",
    "            'interrupted': True,\n",
    "            'games_per_iteration': games_per_iteration,\n",
    "            'training_epochs': training_epochs,\n",
    "            'current_iteration': iteration + 1,\n",
    "            'total_iterations': iterations\n",
    "        }\n",
    "        \n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            iteration=iteration + 1,\n",
    "            total_iterations=iterations,\n",
    "            training_history=training_history,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        print(\"💾 緊急 checkpoint 已保存\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 訓練過程中發生錯誤: {e}\")\n",
    "        # 保存錯誤時的 checkpoint\n",
    "        metadata = {\n",
    "            'error': str(e),\n",
    "            'games_per_iteration': games_per_iteration,\n",
    "            'training_epochs': training_epochs,\n",
    "            'current_iteration': iteration + 1,\n",
    "            'total_iterations': iterations\n",
    "        }\n",
    "        \n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            iteration=iteration + 1,\n",
    "            total_iterations=iterations,\n",
    "            training_history=training_history,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        print(\"💾 錯誤 checkpoint 已保存\")\n",
    "        raise\n",
    "    \n",
    "    # 最終保存\n",
    "    final_metadata = {\n",
    "        'completed': True,\n",
    "        'games_per_iteration': games_per_iteration,\n",
    "        'training_epochs': training_epochs,\n",
    "        'total_iterations': iterations\n",
    "    }\n",
    "    \n",
    "    final_checkpoint_path = checkpoint_manager.save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iteration=iterations,\n",
    "        total_iterations=iterations,\n",
    "        training_history=training_history,\n",
    "        metadata=final_metadata\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 訓練完成！\")\n",
    "    print(f\"📁 最終模型已保存: {final_checkpoint_path}\")\n",
    "    \n",
    "    return model, training_history\n",
    "\n",
    "def train_model_with_history(model, optimizer, states, policies, rewards, epochs=5):\n",
    "    \"\"\"改進的訓練函數，返回詳細的損失歷史\"\"\"\n",
    "    # 創建數據集\n",
    "    X = torch.FloatTensor(states).to(device)\n",
    "    policy_y = torch.FloatTensor(policies).to(device)\n",
    "    value_y = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X, policy_y, value_y)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # 損失函數\n",
    "    policy_criterion = nn.MSELoss()\n",
    "    value_criterion = nn.MSELoss()\n",
    "    \n",
    "    # 記錄每個 epoch 的損失\n",
    "    epoch_losses = {\n",
    "        'total': [],\n",
    "        'policy': [],\n",
    "        'value': []\n",
    "    }\n",
    "    \n",
    "    # 訓練循環\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        policy_loss_sum = 0\n",
    "        value_loss_sum = 0\n",
    "        \n",
    "        for batch_x, batch_policy_y, batch_value_y in dataloader:\n",
    "            # 前向傳播\n",
    "            policy_pred, value_pred = model(batch_x)\n",
    "            \n",
    "            # 計算損失\n",
    "            policy_loss = policy_criterion(policy_pred, batch_policy_y)\n",
    "            value_loss = value_criterion(value_pred, batch_value_y)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            # 反向傳播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 累計損失\n",
    "            total_loss += loss.item()\n",
    "            policy_loss_sum += policy_loss.item()\n",
    "            value_loss_sum += value_loss.item()\n",
    "        \n",
    "        # 計算平均損失\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_policy_loss = policy_loss_sum / len(dataloader)\n",
    "        avg_value_loss = value_loss_sum / len(dataloader)\n",
    "        \n",
    "        # 記錄損失\n",
    "        epoch_losses['total'].append(avg_loss)\n",
    "        epoch_losses['policy'].append(avg_policy_loss)\n",
    "        epoch_losses['value'].append(avg_value_loss)\n",
    "        \n",
    "        # 打印訓練進度\n",
    "        print(f'  Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Policy: {avg_policy_loss:.4f}, Value: {avg_value_loss:.4f}')\n",
    "    \n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6ce8b",
   "metadata": {},
   "source": [
    "## 使用示例\n",
    "\n",
    "以下是訓練與使用強 AI 模型的完整流程示例。由於訓練過程非常耗時，以下代碼默認不會執行完整訓練，而是提供了快速測試和模擬訓練的選項。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c896b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_progress(training_history):\n",
    "    \"\"\"繪製訓練進度圖表\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('AI 訓練進度', fontsize=16)\n",
    "    \n",
    "    # 損失曲線\n",
    "    if training_history['losses']:\n",
    "        axes[0, 0].plot(training_history['losses'], label='總損失', color='red')\n",
    "        axes[0, 0].plot(training_history['policy_losses'], label='策略損失', color='blue')\n",
    "        axes[0, 0].plot(training_history['value_losses'], label='價值損失', color='green')\n",
    "        axes[0, 0].set_title('訓練損失')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('損失')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "    \n",
    "    # 勝率曲線\n",
    "    if training_history['win_rates']:\n",
    "        axes[0, 1].plot(range(2, len(training_history['win_rates']) * 2 + 1, 2), \n",
    "                       training_history['win_rates'], marker='o', color='purple')\n",
    "        axes[0, 1].set_title('對抗隨機 AI 勝率')\n",
    "        axes[0, 1].set_xlabel('迭代')\n",
    "        axes[0, 1].set_ylabel('勝率')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    # 迭代完成情況\n",
    "    if training_history['iterations_completed']:\n",
    "        axes[1, 0].bar(range(len(training_history['iterations_completed'])), \n",
    "                      training_history['iterations_completed'], color='orange')\n",
    "        axes[1, 0].set_title('已完成的迭代')\n",
    "        axes[1, 0].set_xlabel('序號')\n",
    "        axes[1, 0].set_ylabel('迭代次數')\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    # 最近損失趨勢\n",
    "    if len(training_history['losses']) > 10:\n",
    "        recent_losses = training_history['losses'][-50:]  # 最近 50 個 epoch\n",
    "        axes[1, 1].plot(recent_losses, color='darkred')\n",
    "        axes[1, 1].set_title('最近損失趨勢')\n",
    "        axes[1, 1].set_xlabel('最近的 Epoch')\n",
    "        axes[1, 1].set_ylabel('損失')\n",
    "        axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_checkpoint_info():\n",
    "    \"\"\"顯示 checkpoint 資訊\"\"\"\n",
    "    print(\"📋 Checkpoint 管理器狀態\")\n",
    "    print(f\"📁 Checkpoint 目錄: {checkpoint_manager.checkpoint_dir}\")\n",
    "    \n",
    "    # 列出可用的 checkpoint\n",
    "    checkpoints = checkpoint_manager.list_checkpoints()\n",
    "    \n",
    "    # 如果有最新的 checkpoint，顯示其詳細資訊\n",
    "    latest_path = checkpoint_manager.checkpoint_dir / \"latest_checkpoint.pt\"\n",
    "    if latest_path.exists():\n",
    "        print(\"\\n📄 最新 Checkpoint 詳細資訊:\")\n",
    "        checkpoint = torch.load(latest_path, map_location='cpu')\n",
    "        print(f\"  🔢 迭代: {checkpoint['iteration']}/{checkpoint['total_iterations']}\")\n",
    "        print(f\"  📅 時間戳: {checkpoint['timestamp']}\")\n",
    "        \n",
    "        if 'metadata' in checkpoint and checkpoint['metadata']:\n",
    "            print(\"  📋 元數據:\")\n",
    "            for key, value in checkpoint['metadata'].items():\n",
    "                print(f\"    {key}: {value}\")\n",
    "        \n",
    "        if 'training_history' in checkpoint:\n",
    "            history = checkpoint['training_history']\n",
    "            if history['losses']:\n",
    "                print(f\"  📉 最新損失: {history['losses'][-1]:.4f}\")\n",
    "            if history['win_rates']:\n",
    "                print(f\"  🏆 最新勝率: {history['win_rates'][-1]:.2%}\")\n",
    "\n",
    "# 顯示當前 checkpoint 狀態\n",
    "show_checkpoint_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638540c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_checkpoint_info():\n",
    "    \"\"\"顯示當前checkpoint狀態信息\"\"\"\n",
    "    print(\"📊 === Checkpoint 狀態信息 ===\")\n",
    "    \n",
    "    # 顯示checkpoint目錄信息\n",
    "    print(f\"📁 Checkpoint目錄: {checkpoint_manager.checkpoint_dir}\")\n",
    "    \n",
    "    # 獲取最新checkpoint信息\n",
    "    info = checkpoint_manager.get_checkpoint_info()\n",
    "    \n",
    "    if info is None:\n",
    "        print(\"📭 目前沒有可用的checkpoint\")\n",
    "        print(\"🚀 下次訓練將從頭開始\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📈 當前進度: {info['iteration']}/{info['total_iterations']} 迭代\")\n",
    "    print(f\"📅 最後更新: {info['timestamp']}\")\n",
    "    \n",
    "    # 顯示訓練歷史統計\n",
    "    history = info['training_history']\n",
    "    if history.get('losses'):\n",
    "        print(f\"📉 最新損失: {history['losses'][-1]:.4f}\")\n",
    "        print(f\"📊 總訓練epoch: {len(history['losses'])}\")\n",
    "    \n",
    "    if history.get('win_rates'):\n",
    "        print(f\"🏆 最新勝率: {history['win_rates'][-1]:.2%}\")\n",
    "        print(f\"📈 評估次數: {len(history['win_rates'])}\")\n",
    "    \n",
    "    # 顯示元數據\n",
    "    metadata = info.get('metadata', {})\n",
    "    if metadata:\n",
    "        print(\"📋 額外信息:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # 顯示可用checkpoint列表\n",
    "    checkpoint_files = checkpoint_manager.list_checkpoints()\n",
    "    print(f\"💾 可用checkpoint數量: {len(checkpoint_files)}\")\n",
    "    \n",
    "    if checkpoint_files:\n",
    "        print(\"📋 最近的checkpoint:\")\n",
    "        for i, cp_file in enumerate(checkpoint_files[:3]):\n",
    "            size_mb = cp_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   {i+1}. {cp_file.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_strong_ai_with_checkpoint(iterations=10, games_per_iteration=100, \n",
    "                                   training_epochs=5, resume_from_checkpoint=True, \n",
    "                                   save_every=2, fast_test=False):\n",
    "    \"\"\"使用checkpoint功能訓練強力AI\"\"\"\n",
    "    print(f\"🚀 開始訓練強力AI（{'快速測試' if fast_test else '完整訓練'}）\")\n",
    "    print(f\"📊 訓練參數: {iterations}迭代 x {games_per_iteration}場遊戲 x {training_epochs}個epoch\")\n",
    "    \n",
    "    # 初始化模型和優化器\n",
    "    model = ReversiNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 初始化訓練歷史\n",
    "    training_history = {\n",
    "        'losses': [],\n",
    "        'policy_losses': [],\n",
    "        'value_losses': [],\n",
    "        'win_rates': [],\n",
    "        'iterations': []\n",
    "    }\n",
    "    \n",
    "    start_iteration = 0\n",
    "    \n",
    "    # 嘗試載入checkpoint\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = checkpoint_manager.load_latest_checkpoint(model, optimizer)\n",
    "        if checkpoint:\n",
    "            start_iteration = checkpoint['iteration']\n",
    "            training_history = checkpoint.get('training_history', training_history)\n",
    "            print(f\"📥 從迭代 {start_iteration} 繼續訓練\")\n",
    "    \n",
    "    # 如果快速測試，調整參數\n",
    "    if fast_test:\n",
    "        iterations = min(iterations, 2)\n",
    "        games_per_iteration = min(games_per_iteration, 5)\n",
    "        training_epochs = min(training_epochs, 1)\n",
    "        save_every = 1\n",
    "        print(f\"🧪 快速測試模式：{iterations}迭代 x {games_per_iteration}場遊戲 x {training_epochs}個epoch\")\n",
    "    \n",
    "    try:\n",
    "        for iteration in range(start_iteration, iterations):\n",
    "            print(f\"\\n🔄 === 迭代 {iteration+1}/{iterations} ===\")\n",
    "            \n",
    "            # 收集訓練數據\n",
    "            print(\"🎮 收集自我對弈數據...\")\n",
    "            states, policies, rewards = collect_selfplay_data(\n",
    "                model, num_games=games_per_iteration, \n",
    "                use_mcts=(not fast_test), exploration_weight=1.0\n",
    "            )\n",
    "            \n",
    "            print(f\"📚 收集到 {len(states)} 個訓練樣本\")\n",
    "            \n",
    "            # 訓練模型\n",
    "            print(f\"🏋️‍♂️ 開始神經網路訓練（{training_epochs} epochs）...\")\n",
    "            model, epoch_losses = train_model_with_history(\n",
    "                model, optimizer, states, policies, rewards, epochs=training_epochs\n",
    "            )\n",
    "            \n",
    "            # 記錄訓練歷史\n",
    "            if epoch_losses['total']:\n",
    "                training_history['losses'].extend(epoch_losses['total'])\n",
    "                training_history['policy_losses'].extend(epoch_losses['policy'])\n",
    "                training_history['value_losses'].extend(epoch_losses['value'])\n",
    "                training_history['iterations'].extend([iteration+1] * len(epoch_losses['total']))\n",
    "            \n",
    "            # 評估模型（每2個迭代或在快速測試中每次都評估）\n",
    "            if (iteration + 1) % 2 == 0 or fast_test:\n",
    "                print(\"\\n📈 評估模型性能...\")\n",
    "                num_eval_games = 10 if fast_test else 50\n",
    "                win_rate, _, _ = evaluate_against_random(model, num_games=num_eval_games)\n",
    "                training_history['win_rates'].append(win_rate)\n",
    "                print(f\"🎯 當前勝率: {win_rate:.2%}\")\n",
    "                \n",
    "                # 如果勝率很高，可以提前結束快速測試\n",
    "                if fast_test and win_rate > 0.9:\n",
    "                    print(\"✅ 快速測試：勝率已達90%，測試成功！\")\n",
    "                    break\n",
    "            \n",
    "            # 保存checkpoint\n",
    "            if (iteration + 1) % save_every == 0:\n",
    "                metadata = {\n",
    "                    'games_per_iteration': games_per_iteration,\n",
    "                    'training_epochs': training_epochs,\n",
    "                    'current_iteration': iteration + 1,\n",
    "                    'total_iterations': iterations,\n",
    "                    'fast_test': fast_test\n",
    "                }\n",
    "                \n",
    "                checkpoint_manager.save_checkpoint(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    iteration=iteration + 1,\n",
    "                    total_iterations=iterations,\n",
    "                    training_history=training_history,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "            \n",
    "            # 定期清理舊checkpoint\n",
    "            if (iteration + 1) % 3 == 0:\n",
    "                checkpoint_manager.clean_old_checkpoints(keep_count=5)\n",
    "            \n",
    "            # 顯示進度\n",
    "            progress = (iteration + 1) / iterations * 100\n",
    "            print(f\"\\n✅ 迭代 {iteration+1}/{iterations} 完成 ({progress:.1f}%)\")\n",
    "            if training_history['losses']:\n",
    "                recent_loss = training_history['losses'][-1]\n",
    "                print(f\"📉 最近損失: {recent_loss:.4f}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️  訓練被用戶中斷\")\n",
    "        # 保存中斷checkpoint\n",
    "        metadata = {\n",
    "            'interrupted': True,\n",
    "            'games_per_iteration': games_per_iteration,\n",
    "            'training_epochs': training_epochs,\n",
    "            'current_iteration': iteration + 1,\n",
    "            'total_iterations': iterations,\n",
    "            'fast_test': fast_test\n",
    "        }\n",
    "        \n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            iteration=iteration + 1,\n",
    "            total_iterations=iterations,\n",
    "            training_history=training_history,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        print(\"💾 緊急checkpoint已保存\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 訓練過程中發生錯誤: {e}\")\n",
    "        # 保存錯誤checkpoint\n",
    "        metadata = {\n",
    "            'error': str(e),\n",
    "            'games_per_iteration': games_per_iteration,\n",
    "            'training_epochs': training_epochs,\n",
    "            'current_iteration': iteration + 1,\n",
    "            'total_iterations': iterations,\n",
    "            'fast_test': fast_test\n",
    "        }\n",
    "        \n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            iteration=iteration + 1,\n",
    "            total_iterations=iterations,\n",
    "            training_history=training_history,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        print(\"💾 錯誤checkpoint已保存\")\n",
    "        raise\n",
    "    \n",
    "    # 保存最終checkpoint\n",
    "    final_metadata = {\n",
    "        'completed': True,\n",
    "        'games_per_iteration': games_per_iteration,\n",
    "        'training_epochs': training_epochs,\n",
    "        'total_iterations': iterations,\n",
    "        'fast_test': fast_test\n",
    "    }\n",
    "    \n",
    "    final_checkpoint_path = checkpoint_manager.save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iteration=iterations,\n",
    "        total_iterations=iterations,\n",
    "        training_history=training_history,\n",
    "        metadata=final_metadata\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 訓練完成！\")\n",
    "    print(f\"📁 最終模型已保存: {final_checkpoint_path}\")\n",
    "    \n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置訓練參數\n",
    "fast_test = True  # 改為 False 以執行完整訓練\n",
    "\n",
    "if fast_test:\n",
    "    # 快速測試 - 訓練極小版本以驗證代碼可用\n",
    "    print(\"執行快速測試...\")\n",
    "    \n",
    "    # 初始化模型\n",
    "    test_model = ReversiNet().to(device)\n",
    "    \n",
    "    # 生成少量訓練數據\n",
    "    print(\"生成少量訓練數據...\")\n",
    "    states, policies, rewards = generate_training_data(test_model, num_games=2)\n",
    "    \n",
    "    # 測試訓練\n",
    "    print(\"\\n測試訓練模型...\")\n",
    "    test_model = train_model(test_model, states, policies, rewards, epochs=1)\n",
    "    \n",
    "    # 測試轉換為 JS 格式\n",
    "    print(\"\\n測試轉換為 JavaScript 格式...\")\n",
    "    convert_model_to_js(test_model, \"test_reversi_model.json\")\n",
    "    \n",
    "    print(\"\\n快速測試完成。若要執行完整訓練，請將 fast_test 設置為 False\")\n",
    "else:\n",
    "    # 完整訓練過程\n",
    "    print(\"開始完整訓練過程...\")\n",
    "    \n",
    "    # 訓練強力 AI\n",
    "    strong_model = train_strong_ai(iterations=10, games_per_iteration=100, training_epochs=5)\n",
    "    \n",
    "    # 評估模型\n",
    "    print(\"\\n評估模型對抗隨機 AI...\")\n",
    "    evaluate_against_random(strong_model, num_games=50)\n",
    "    \n",
    "    # 轉換為 JavaScript 格式\n",
    "    print(\"\\n將模型轉換為 JavaScript 格式...\")\n",
    "    convert_model_to_js(strong_model, \"strong_reversi_model.json\")\n",
    "    \n",
    "    print(\"\\n訓練完成！模型已保存並轉換為 JavaScript 格式。\")\n",
    "\n",
    "def plot_training_progress(training_history):\n",
    "    \"\"\"繪製詳細的訓練進度圖表\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('🤖 AI 訓練進度監控', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 損失曲線\n",
    "    if training_history.get('losses'):\n",
    "        axes[0, 0].plot(training_history['losses'], label='📉 總損失', color='red', linewidth=2)\n",
    "        \n",
    "        if training_history.get('policy_losses'):\n",
    "            axes[0, 0].plot(training_history['policy_losses'], label='🎯 策略損失', \n",
    "                           color='blue', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "        if training_history.get('value_losses'):\n",
    "            axes[0, 0].plot(training_history['value_losses'], label='💎 價值損失', \n",
    "                           color='green', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "        axes[0, 0].set_title('📈 訓練損失變化', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('損失值')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_yscale('log')  # 使用對數尺度更好觀察損失變化\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, '暫無損失數據', ha='center', va='center', fontsize=12)\n",
    "        axes[0, 0].set_title('📈 訓練損失變化')\n",
    "    \n",
    "    # 勝率曲線\n",
    "    if training_history.get('win_rates'):\n",
    "        evaluation_points = list(range(2, len(training_history['win_rates']) * 2 + 1, 2))\n",
    "        axes[0, 1].plot(evaluation_points, training_history['win_rates'], \n",
    "                       marker='o', markersize=8, color='purple', linewidth=2, \n",
    "                       markerfacecolor='gold', markeredgecolor='purple')\n",
    "        \n",
    "        # 添加目標線\n",
    "        axes[0, 1].axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='🎯 目標勝率 (90%)')\n",
    "        axes[0, 1].axhline(y=0.7, color='orange', linestyle='--', alpha=0.7, label='📊 良好勝率 (70%)')\n",
    "        \n",
    "        axes[0, 1].set_title('🏆 對抗隨機AI勝率', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('迭代次數')\n",
    "        axes[0, 1].set_ylabel('勝率')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 添加數值標籤\n",
    "        for i, (x, y) in enumerate(zip(evaluation_points, training_history['win_rates'])):\n",
    "            axes[0, 1].annotate(f'{y:.2%}', (x, y), textcoords=\"offset points\", \n",
    "                               xytext=(0,10), ha='center', fontsize=9)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, '暫無勝率數據', ha='center', va='center', fontsize=12)\n",
    "        axes[0, 1].set_title('🏆 對抗隨機AI勝率')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # 訓練進度統計\n",
    "    axes[1, 0].axis('off')\n",
    "    stats_text = \"📊 訓練統計:\\n\\n\"\n",
    "    \n",
    "    if training_history.get('losses'):\n",
    "        stats_text += f\"📉 總訓練epochs: {len(training_history['losses'])}\\n\"\n",
    "        stats_text += f\"🔻 最低損失: {min(training_history['losses']):.4f}\\n\"\n",
    "        stats_text += f\"📈 最新損失: {training_history['losses'][-1]:.4f}\\n\\n\"\n",
    "    \n",
    "    if training_history.get('win_rates'):\n",
    "        stats_text += f\"🏆 最高勝率: {max(training_history['win_rates']):.2%}\\n\"\n",
    "        stats_text += f\"📊 最新勝率: {training_history['win_rates'][-1]:.2%}\\n\"\n",
    "        stats_text += f\"📈 評估次數: {len(training_history['win_rates'])}\\n\\n\"\n",
    "    \n",
    "    # 添加checkpoint信息\n",
    "    checkpoint_info = checkpoint_manager.get_checkpoint_info()\n",
    "    if checkpoint_info:\n",
    "        stats_text += f\"💾 Checkpoint狀態:\\n\"\n",
    "        stats_text += f\"   迭代: {checkpoint_info['iteration']}/{checkpoint_info['total_iterations']}\\n\"\n",
    "        stats_text += f\"   更新時間: {checkpoint_info['timestamp']}\\n\"\n",
    "    \n",
    "    axes[1, 0].text(0.05, 0.95, stats_text, transform=axes[1, 0].transAxes, \n",
    "                    fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    # 損失分佈直方圖（如果有足夠數據）\n",
    "    if training_history.get('losses') and len(training_history['losses']) > 10:\n",
    "        recent_losses = training_history['losses'][-50:]  # 最近50個epoch的損失\n",
    "        axes[1, 1].hist(recent_losses, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1, 1].axvline(np.mean(recent_losses), color='red', linestyle='--', \n",
    "                          label=f'平均值: {np.mean(recent_losses):.4f}')\n",
    "        axes[1, 1].set_title('📊 最近損失分佈', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('損失值')\n",
    "        axes[1, 1].set_ylabel('頻次')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, '需要更多數據\\n才能顯示分佈', ha='center', va='center', fontsize=12)\n",
    "        axes[1, 1].set_title('📊 最近損失分佈')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 顯示詳細的進度報告\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📊 詳細訓練報告\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if training_history.get('losses'):\n",
    "        print(f\"📉 損失趨勢: {training_history['losses'][0]:.4f} → {training_history['losses'][-1]:.4f}\")\n",
    "        \n",
    "        # 計算改善率\n",
    "        if len(training_history['losses']) > 1:\n",
    "            improvement = (training_history['losses'][0] - training_history['losses'][-1]) / training_history['losses'][0] * 100\n",
    "            print(f\"📈 損失改善: {improvement:.1f}%\")\n",
    "    \n",
    "    if training_history.get('win_rates'):\n",
    "        print(f\"🏆 勝率趨勢: {training_history['win_rates'][0]:.2%} → {training_history['win_rates'][-1]:.2%}\")\n",
    "        \n",
    "        # 勝率穩定性\n",
    "        if len(training_history['win_rates']) > 3:\n",
    "            recent_std = np.std(training_history['win_rates'][-3:])\n",
    "            print(f\"📊 最近勝率穩定性: ±{recent_std:.2%}\")\n",
    "    \n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6c3d9",
   "metadata": {},
   "source": [
    "## 結論與使用說明\n",
    "\n",
    "我們已經成功實現了一個基於深度學習的黑白棋 AI 模型，它通過自我對弈和強化學習來提升棋力，使其非常不容易輸。\n",
    "\n",
    "### 如何使用訓練好的模型\n",
    "\n",
    "1. 執行完整訓練過程（將 `fast_test` 設置為 `False`）\n",
    "2. 訓練完成後，你將得到兩個文件：\n",
    "   - `strong_reversi_model.json`：包含模型權重的 JSON 文件\n",
    "   - `reversi_ai.js`：JavaScript 包裝器，用於在瀏覽器中加載和使用模型\n",
    "\n",
    "3. 在你的 Web 應用中使用：\n",
    "   ```javascript\n",
    "   // 初始化 AI\n",
    "   const ai = new ReversiAI();\n",
    "   \n",
    "   // 載入模型權重\n",
    "   await ai.loadWeights('strong_reversi_model.json');\n",
    "   \n",
    "   // 獲取最佳走步\n",
    "   const bestMove = ai.getBestMove(boardState, currentPlayer);\n",
    "   ```\n",
    "\n",
    "### 進一步改進\n",
    "\n",
    "- 增加訓練迭代次數和自我對弈的遊戲數量\n",
    "- 調整神經網路架構，增加網路的深度和寬度\n",
    "- 實現更複雜的搜索算法，如 AlphaZero 中的 PUCT 算法\n",
    "- 添加棋盤狀態增強，通過旋轉和鏡像來增加訓練數據的多樣性\n",
    "\n",
    "## 訓練過程詳解\n",
    "\n",
    "### 設置訓練參數\n",
    "\n",
    "```python\n",
    "fast_test = True  # 改為 False 以執行完整訓練\n",
    "resume_training = True  # 是否從 checkpoint 恢復訓練\n",
    "```\n",
    "\n",
    "### 快速測試\n",
    "\n",
    "當 `fast_test` 設置為 `True` 時，系統將執行快速測試，這是一個訓練極小版本的過程，主要用於驗證代碼的可用性。\n",
    "\n",
    "- 執行快速測試的步驟：\n",
    "  1. 顯示當前的 checkpoint 狀態\n",
    "  2. 使用測試參數進行訓練\n",
    "  3. 顯示訓練進度\n",
    "  4. 測試模型轉換為 JavaScript 格式\n",
    "\n",
    "### 完整訓練過程\n",
    "\n",
    "當 `fast_test` 設置為 `False` 時，系統將執行完整的訓練過程。\n",
    "\n",
    "- 完整訓練過程包括：\n",
    "  1. 顯示當前的 checkpoint 狀態\n",
    "  2. 訓練強力 AI，支援從 checkpoint 恢復訓練\n",
    "  3. 顯示訓練進度\n",
    "  4. 最終評估模型\n",
    "  5. 將模型轉換為 JavaScript 格式\n",
    "  6. 保存最終訓練歷史\n",
    "  7. 清理舊的 checkpoint\n",
    "\n",
    "### 代碼範例\n",
    "\n",
    "以下是訓練過程的主要代碼範例：\n",
    "\n",
    "```python\n",
    "if fast_test:\n",
    "    # 快速測試 - 訓練極小版本以驗證代碼可用\n",
    "    print(\"🚀 執行快速測試...\")\n",
    "    \n",
    "    # 顯示 checkpoint 狀態\n",
    "    show_checkpoint_info()\n",
    "    \n",
    "    # 使用 checkpoint 功能進行測試訓練\n",
    "    print(\"\\n🧪 開始測試訓練（包含 checkpoint 功能）...\")\n",
    "    test_model, test_history = train_strong_ai_with_checkpoint(\n",
    "        iterations=2,  # 只執行 2 個迭代\n",
    "        games_per_iteration=5,  # 每次迭代只玩 5 場遊戲\n",
    "        training_epochs=1,  # 每次只訓練 1 個 epoch\n",
    "        resume_from_checkpoint=resume_training,\n",
    "        save_every=1  # 每個迭代都保存\n",
    "    )\n",
    "    \n",
    "    # 顯示訓練進度\n",
    "    if test_history['losses']:\n",
    "        plot_training_progress(test_history)\n",
    "    \n",
    "    # 測試轉換為 JS 格式\n",
    "    print(\"\\n🔄 測試轉換為 JavaScript 格式...\")\n",
    "    convert_model_to_js(test_model, \"test_reversi_model.json\")\n",
    "    \n",
    "    print(\"\\n✅ 快速測試完成。若要執行完整訓練，請將 fast_test 設置為 False\")\n",
    "    print(\"💡 提示: 如果中斷訓練，下次運行時會自動從 checkpoint 恢復\")\n",
    "    \n",
    "else:\n",
    "    # 完整訓練過程\n",
    "    print(\"🏋️‍♂️ 開始完整訓練過程...\")\n",
    "    \n",
    "    # 顯示當前 checkpoint 狀態\n",
    "    show_checkpoint_info()\n",
    "    \n",
    "    # 訓練強力 AI（支援 checkpoint）\n",
    "    print(\"\\n🤖 開始訓練強力 AI...\")\n",
    "    strong_model, training_history = train_strong_ai_with_checkpoint(\n",
    "        iterations=15,  # 增加迭代次數\n",
    "        games_per_iteration=100,  # 每次迭代 100 場遊戲\n",
    "        training_epochs=5,  # 每次訓練 5 個 epoch\n",
    "        resume_from_checkpoint=resume_training,\n",
    "        save_every=2  # 每 2 個迭代保存一次\n",
    "    )\n",
    "    \n",
    "    # 顯示完整的訓練進度\n",
    "    print(\"\\n📊 顯示訓練進度...\")\n",
    "    plot_training_progress(training_history)\n",
    "    \n",
    "    # 最終評估模型\n",
    "    print(\"\\n🏆 最終評估模型對抗隨機 AI...\")\n",
    "    final_win_rate, _, _ = evaluate_against_random(strong_model, num_games=100)\n",
    "    print(f\"🎯 最終勝率: {final_win_rate:.2%}\")\n",
    "    \n",
    "    # 轉換為 JavaScript 格式\n",
    "    print(\"\\n🔄 將模型轉換為 JavaScript 格式...\")\n",
    "    convert_model_to_js(strong_model, \"strong_reversi_model.json\")\n",
    "    \n",
    "    # 保存最終訓練歷史\n",
    "    with open(checkpoint_manager.checkpoint_dir / \"final_training_history.pkl\", 'wb') as f:\n",
    "        pickle.dump(training_history, f)\n",
    "    \n",
    "    print(\"\\n🎉 訓練完成！模型已保存並轉換為 JavaScript 格式。\")\n",
    "    print(f\"📁 所有檔案已保存到: {checkpoint_manager.checkpoint_dir}\")\n",
    "    \n",
    "    # 最後清理舊的 checkpoint\n",
    "    checkpoint_manager.clean_old_checkpoints(keep_count=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61003f05",
   "metadata": {},
   "source": [
    "## 結論與使用說明\n",
    "\n",
    "我們已經成功實現了一個基於深度學習的黑白棋 AI 模型，具備完整的 checkpoint 功能，讓訓練過程更加安全可靠。\n",
    "\n",
    "### 🔄 Checkpoint 功能特點\n",
    "\n",
    "1. **自動保存**: 訓練過程中定期自動保存進度\n",
    "2. **斷點恢復**: 如果訓練中斷，可以從最後一個 checkpoint 恢復\n",
    "3. **進度追蹤**: 詳細記錄訓練歷史和性能指標\n",
    "4. **錯誤恢復**: 即使發生錯誤也會保存當前狀態\n",
    "5. **空間管理**: 自動清理舊的 checkpoint，節省存儲空間\n",
    "\n",
    "### 🚀 在 Google Colab 中使用\n",
    "\n",
    "1. **首次運行**: 直接執行所有 cell，訓練會自動開始\n",
    "2. **中斷恢復**: 如果訓練中斷，重新運行時會自動從 checkpoint 恢復\n",
    "3. **進度查看**: 隨時查看 `show_checkpoint_info()` 了解當前狀態\n",
    "4. **可視化**: 使用 `plot_training_progress()` 查看訓練曲線\n",
    "\n",
    "### 📁 檔案結構\n",
    "\n",
    "```\n",
    "checkpoints/\n",
    "├── latest_checkpoint.pt          # 最新的 checkpoint\n",
    "├── checkpoint_iter_X_timestamp.pt # 特定迭代的 checkpoint\n",
    "├── final_training_history.pkl     # 完整訓練歷史\n",
    "└── strong_reversi_model.json      # 最終的 JavaScript 模型\n",
    "```\n",
    "\n",
    "### 🛠️ 如何使用訓練好的模型\n",
    "\n",
    "1. 執行完整訓練過程（將 `fast_test` 設置為 `False`）\n",
    "2. 訓練完成後，你將得到：\n",
    "   - `strong_reversi_model.json`：包含模型權重的 JSON 文件\n",
    "   - `reversi_ai.js`：JavaScript 包裝器\n",
    "   - 完整的 checkpoint 檔案用於未來繼續訓練\n",
    "\n",
    "3. 在你的 Web 應用中使用：\n",
    "   ```javascript\n",
    "   // 初始化 AI\n",
    "   const ai = new ReversiAI();\n",
    "   \n",
    "   // 載入模型權重\n",
    "   await ai.loadWeights('strong_reversi_model.json');\n",
    "   \n",
    "   // 獲取最佳走步\n",
    "   const bestMove = ai.getBestMove(boardState, currentPlayer);\n",
    "   ```\n",
    "\n",
    "### 💡 訓練建議\n",
    "\n",
    "- **初學者**: 使用 `fast_test=True` 先熟悉流程\n",
    "- **正式訓練**: 設置 `fast_test=False`，迭代次數建議 15-30 次\n",
    "- **長時間訓練**: 在 Colab Pro 中運行，避免免費版的時間限制\n",
    "- **定期檢查**: 使用可視化功能監控訓練進度\n",
    "\n",
    "### 🔧 進一步改進\n",
    "\n",
    "- 增加訓練迭代次數和自我對弈的遊戲數量\n",
    "- 調整神經網路架構，增加網路的深度和寬度\n",
    "- 實現更複雜的搜索算法，如 AlphaZero 中的 PUCT 算法\n",
    "- 添加棋盤狀態增強，通過旋轉和鏡像來增加訓練數據的多樣性\n",
    "- 實現對抗不同強度 AI 的評估系統\n",
    "\n",
    "### ⚠️ 注意事項\n",
    "\n",
    "- 訓練過程耗時較長，建議在有穩定網路的環境中進行\n",
    "- 在 Colab 中訓練時，checkpoint 會自動保存到 Google Drive\n",
    "- 定期下載重要的 checkpoint 到本地作為備份\n",
    "- 如果記憶體不足，可以減少 `games_per_iteration` 或 `batch_size`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
